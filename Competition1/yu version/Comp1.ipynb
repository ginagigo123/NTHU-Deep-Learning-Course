{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id  Popularity                                       Page content\n",
      "0   0          -1  <html><head><div class=\"article-info\"> <span c...\n",
      "1   1           1  <html><head><div class=\"article-info\"><span cl...\n",
      "2   2           1  <html><head><div class=\"article-info\"><span cl...\n",
      "3   3          -1  <html><head><div class=\"article-info\"><span cl...\n",
      "4   4          -1  <html><head><div class=\"article-info\"><span cl...\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./data/comp1/train.csv')\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def preprocessor(text):\n",
    "    # remove HTML tags\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    \n",
    "    # regex for matching emoticons, keep emoticons, ex: :), :-P, :-D\n",
    "    r = '(?::|;|=|X)(?:-)?(?:\\)|\\(|D|P)'\n",
    "    emoticons = re.findall(r, text)\n",
    "    text = re.sub(r, '', text)\n",
    "    \n",
    "    # convert to lowercase and append all emoticons behind (with space in between)\n",
    "    # replace('-','') removes nose of emoticons\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-','')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['runner', 'like', 'run', 'thu', 'run']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def tokenizer_stem_nostop(text):\n",
    "    porter = PorterStemmer()\n",
    "    return [porter.stem(w) for w in re.split('\\s+', text.strip()) \\\n",
    "            if w not in stop and re.match('[a-zA-Z]+', w)]\n",
    "\n",
    "print(tokenizer_stem_nostop('runners like running and thus they run'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  Id  Popularity            author                 time  \\\n",
      "0           0   0          -1               NaN  2013-06-19 15:04:30   \n",
      "1           1   1           1  Christina Warren  2013-03-28 17:40:55   \n",
      "2           2   2           1         Sam Laird  2014-05-07 19:15:20   \n",
      "3           3   3          -1         Sam Laird  2013-10-11 02:26:50   \n",
      "4           4   4          -1   Connor Finnegan  2014-04-17 03:31:43   \n",
      "\n",
      "                                               topic  image num  video num  \\\n",
      "0  ['Asteroid', 'Asteroids', 'challenge', 'Earth'...          1          0   \n",
      "1  ['Apps and Software', 'Google', 'open source',...          2          0   \n",
      "2  ['Entertainment', 'NFL', 'NFL Draft', 'Sports'...          2         25   \n",
      "3       ['Sports', 'Video', 'Videos', 'Watercooler']          1         21   \n",
      "4  ['Entertainment', 'instagram', 'instagram vide...         52          1   \n",
      "\n",
      "                                                body  week of day  year  \\\n",
      "0  NASA's Grand Challenge: Stop Asteroids From De...            3  2013   \n",
      "1  Google's New Open Source Patent Pledge: We Won...            4  2013   \n",
      "2  Ballin': 2014 NFL Draft Picks Get to Choose Th...            3  2014   \n",
      "3  Cameraperson Fails Deliver Slapstick Laughs   ...            5  2013   \n",
      "4  NFL Star Helps Young Fan Prove Friendship With...            4  2014   \n",
      "\n",
      "   month  hour  \n",
      "0      6    15  \n",
      "1      3    17  \n",
      "2      5    19  \n",
      "3     10     2  \n",
      "4      4     3  \n"
     ]
    }
   ],
   "source": [
    "df_new = pd.read_csv('./data/comp1/train_new_feature.csv')\n",
    "print(df_new.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack, vstack\n",
    "import scipy.sparse\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "\n",
    "def preprocess_new_feature(df_new,sample_size):\n",
    "    ## body\n",
    "    doc_body = df_new['body'].iloc[:sample_size]\n",
    "    hashvec = HashingVectorizer(n_features=2**10,\n",
    "                                preprocessor=preprocessor,\n",
    "                                tokenizer=tokenizer_stem_nostop)\n",
    "    doc_hash = hashvec.transform(doc_body)\n",
    "    print(\"body\",doc_hash.shape,type(doc_hash))\n",
    "\n",
    "    ## topic\n",
    "    ###########\n",
    "    tmp = df_new['topic'].iloc[:sample_size]\n",
    "    doc_topic = []\n",
    "    for i in tmp:\n",
    "        doc_topic.append(i.replace('[', '').replace(']', '').replace(',', '').replace('\\'', ''))\n",
    "    hashvec = HashingVectorizer(n_features=2**10,\n",
    "                                preprocessor=preprocessor,\n",
    "                                tokenizer=tokenizer_stem_nostop)\n",
    "    doc_hash = hstack([doc_hash,hashvec.transform(doc_topic)])\n",
    "    ###########\n",
    "    ###########\n",
    "    # tmp = df_new['topic'].iloc[:sample_size]\n",
    "    # topic_set = set()\n",
    "\n",
    "    # for i in tmp:\n",
    "    #     #print(i)\n",
    "    #     items = i.replace('[', '#').replace(']', '#').replace(',', '#').replace('\\\"', '#').replace('\\'', '@').replace(' @', '')\n",
    "    #     items = items.split('#')\n",
    "    #     item_tmp = []\n",
    "    #     for item in items:\n",
    "    #         item_tmp.append(item.replace('@s', '$').replace('@', '').replace('$', '\\'s'))\n",
    "    #     #print(item_tmp)\n",
    "    #     topic_set.update(item_tmp)\n",
    "\n",
    "    # topic_list = list(topic_set)[1:] # [0] is empty: ''\n",
    "\n",
    "    # doc_topic = []\n",
    "    # for i in tmp:\n",
    "    #     items = i.replace('[', '#').replace(']', '#').replace(',', '#').replace('\\\"', '#').replace('\\'', '@').replace(' @', '')\n",
    "    #     items = items.split('#')\n",
    "    #     item_tmp = []\n",
    "    #     for item in items:\n",
    "    #         item_tmp.append(item.replace('@s', '$').replace('@', '').replace('$', '\\'s'))\n",
    "    #     topic_ids = []\n",
    "    #     for tag in item_tmp:\n",
    "    #         if tag != '':\n",
    "    #             topic_ids.append(topic_list.index(tag))\n",
    "    #     topic_ids.sort()\n",
    "    #     doc_topic.append(topic_ids)\n",
    "\n",
    "    # # print(topic_list)\n",
    "    # pad = len(max(doc_topic, key=len))\n",
    "    # doc_topic = np.array([i + [0]*(pad-len(i)) for i in doc_topic])\n",
    "    # doc_hash = hstack([doc_hash,doc_topic])\n",
    "    ###########\n",
    "    print(\"topic\",doc_hash.shape,type(doc_hash))\n",
    "\n",
    "    # ## time\n",
    "    # doc_time = df_new['time'].iloc[:100]\n",
    "    # hashvec = HashingVectorizer(n_features=2**10,\n",
    "    #                             preprocessor=preprocessor,\n",
    "    #                             tokenizer=tokenizer_stem_nostop)\n",
    "    # doc_hash = hstack([doc_hash,hashvec.transform(doc_time)])\n",
    "    # print(doc_hash.shape,type(doc_hash))\n",
    "\n",
    "    ## author\n",
    "    tmp = df_new['author'].replace(np.nan, 'x').iloc[:sample_size]\n",
    "    unique = list(set(tmp))\n",
    "    doc_author = []\n",
    "    for i in tmp:\n",
    "        doc_author.append(unique.index(i))\n",
    "    # print(doc_author)\n",
    "    doc_author = np.array(doc_author)\n",
    "    doc_hash = hstack([doc_hash,doc_author.reshape((sample_size,1))])\n",
    "    print(\"author\",doc_hash.shape,type(doc_hash))\n",
    "\n",
    "    ## img\n",
    "    doc_img = df_new['image num'].iloc[:sample_size].to_numpy()\n",
    "    doc_hash =hstack([doc_hash,doc_img.reshape((sample_size,1))])\n",
    "    print(\"img num\",doc_hash.shape,type(doc_hash))\n",
    "\n",
    "    ## video\n",
    "    doc_video = df_new['video num'].iloc[:sample_size].to_numpy()\n",
    "    doc_hash = hstack([doc_hash,doc_video.reshape((sample_size,1))])\n",
    "    print(\"video num\",doc_hash.shape,type(doc_hash))\n",
    "\n",
    "    ## week of day\n",
    "    doc_w = df_new['week of day'].iloc[:sample_size].to_numpy()\n",
    "    doc_hash = hstack([doc_hash,doc_w.reshape((sample_size,1))])\n",
    "    print(\"week of day\",doc_hash.shape,type(doc_hash))\n",
    "\n",
    "    ## weekend\n",
    "    doc_w = df_new['week of day'].iloc[:sample_size].to_numpy()\n",
    "    doc_weekend = []\n",
    "    for i in doc_w:\n",
    "        if i > 5:\n",
    "            doc_weekend.append(1)\n",
    "        else:\n",
    "            doc_weekend.append(0)\n",
    "    doc_weekend = np.array(doc_weekend)\n",
    "    doc_hash = hstack([doc_hash,doc_weekend.reshape((sample_size,1))])\n",
    "    print(\"weekend\",doc_hash.shape,type(doc_hash))\n",
    "\n",
    "    ## month\n",
    "    doc_m = df_new['month'].iloc[:sample_size].to_numpy()\n",
    "    doc_hash = hstack([doc_hash,doc_m.reshape((sample_size,1))])\n",
    "    print(\"month\",doc_hash.shape,type(doc_hash))\n",
    "\n",
    "    ## hour\n",
    "    doc_h = df_new['hour'].iloc[:sample_size].to_numpy()\n",
    "    doc_hash = hstack([doc_hash,doc_h.reshape((sample_size,1))])\n",
    "    print(\"hour\",doc_hash.shape,type(doc_hash))\n",
    "\n",
    "    ## year\n",
    "    doc_y = df_new['year'].iloc[:sample_size].to_numpy()\n",
    "    doc_hash = hstack([doc_hash,doc_y.reshape((sample_size,1))])\n",
    "    print(\"year\",doc_hash.shape,type(doc_hash))\n",
    "\n",
    "    # print(doc_hash.todense())\n",
    "    return doc_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "body (27643, 1024) <class 'scipy.sparse.csr.csr_matrix'>\n",
      "topic (27643, 2048) <class 'scipy.sparse.coo.coo_matrix'>\n",
      "author (27643, 2049) <class 'scipy.sparse.coo.coo_matrix'>\n",
      "img num (27643, 2050) <class 'scipy.sparse.coo.coo_matrix'>\n",
      "video num (27643, 2051) <class 'scipy.sparse.coo.coo_matrix'>\n",
      "week of day (27643, 2052) <class 'scipy.sparse.coo.coo_matrix'>\n",
      "weekend (27643, 2053) <class 'scipy.sparse.coo.coo_matrix'>\n",
      "month (27643, 2054) <class 'scipy.sparse.coo.coo_matrix'>\n",
      "hour (27643, 2055) <class 'scipy.sparse.coo.coo_matrix'>\n",
      "year (27643, 2056) <class 'scipy.sparse.coo.coo_matrix'>\n",
      "(27643,)\n"
     ]
    }
   ],
   "source": [
    "sample_size = 27643 # 100 #\n",
    "doc_hash = preprocess_new_feature(df_new,sample_size)\n",
    "doc_y_label = df_new['Popularity'].iloc[:sample_size].to_numpy()\n",
    "print(doc_y_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10], 'probability': [True, True]}\n",
    "# svc = svm.SVC()\n",
    "# clf = GridSearchCV(svc, parameters, n_jobs=-1, cv=10, scoring='roc_auc')\n",
    "rf = RandomForestClassifier()\n",
    "parameters = {'n_estimators':[1,10,50,100]}\n",
    "clf = GridSearchCV(rf, parameters, n_jobs=-1, cv=10, scoring='roc_auc')\n",
    "\n",
    "clf.fit(doc_hash, doc_y_label)\n",
    "print(clf.best_params_, \"score: \", clf.best_score_)\n",
    "print(clf.result_)\n",
    "\n",
    "# scores = cross_val_score(estimator=RandomForestRegressor(),\n",
    "#                          X=doc_hash, y=doc_y_label,\n",
    "#                          cv=10, scoring='roc_auc')\n",
    "# print('%.3f (+/-%.3f)' % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0     Id                                       Page content  \\\n",
      "0           0  27643  <html><head><div class=\"article-info\"><span cl...   \n",
      "1           1  27644  <html><head><div class=\"article-info\"><span cl...   \n",
      "2           2  27645  <html><head><div class=\"article-info\"><span cl...   \n",
      "3           3  27646  <html><head><div class=\"article-info\"><span cl...   \n",
      "4           4  27647  <html><head><div class=\"article-info\"><span cl...   \n",
      "\n",
      "           author                 time  \\\n",
      "0       Sam Laird  2013-09-09 19:47:02   \n",
      "1  Stan Schroeder  2013-10-31 09:25:02   \n",
      "2  Todd Wasserman  2013-06-25 12:54:54   \n",
      "3    Neha Prakash  2013-02-13 03:30:21   \n",
      "4     Josh Dickey  2014-10-03 01:34:54   \n",
      "\n",
      "                                               topic  image num  video num  \\\n",
      "0  ['Entertainment', 'Music', 'One Direction', 's...          1          7   \n",
      "1  ['Gadgets', 'glass', 'Google', 'Google Glass',...          3          0   \n",
      "2  ['amazon', 'amazon kindle', 'Business', 'Gaming']          2          0   \n",
      "3  ['Between Two Ferns', 'Movies', 'The Oscars', ...          1          4   \n",
      "4  ['American Sniper', 'Awards', 'Bradley Cooper'...          1          1   \n",
      "\n",
      "                                                body  week of day    year  \\\n",
      "0  Soccer Star Gets Twitter Death Threats After T...          1.0  2013.0   \n",
      "1  Google Glass Gets an Accessory Store Shortly a...          4.0  2013.0   \n",
      "2  OUYA Gaming Console Already Sold Out on Amazon...          2.0  2013.0   \n",
      "3  'Between Two Ferns' Mocks Oscar Nominees  Betw...          3.0  2013.0   \n",
      "4  'American Sniper' Trailer: Looks Like Eastwood...          5.0  2014.0   \n",
      "\n",
      "   month  hour  \n",
      "0    9.0  19.0  \n",
      "1   10.0   9.0  \n",
      "2    6.0  12.0  \n",
      "3    2.0   3.0  \n",
      "4   10.0   1.0   (11847, 13)\n"
     ]
    }
   ],
   "source": [
    "df_new_test = pd.read_csv('./data/comp1/test_feature.csv')\n",
    "print(df_new_test.head(5),df_new_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "body (11847, 1024) <class 'scipy.sparse.csr.csr_matrix'>\n",
      "topic (11847, 2048) <class 'scipy.sparse.coo.coo_matrix'>\n",
      "author (11847, 2049) <class 'scipy.sparse.coo.coo_matrix'>\n",
      "img num (11847, 2050) <class 'scipy.sparse.coo.coo_matrix'>\n",
      "video num (11847, 2051) <class 'scipy.sparse.coo.coo_matrix'>\n",
      "week of day (11847, 2052) <class 'scipy.sparse.coo.coo_matrix'>\n",
      "weekend (11847, 2053) <class 'scipy.sparse.coo.coo_matrix'>\n",
      "month (11847, 2054) <class 'scipy.sparse.coo.coo_matrix'>\n",
      "hour (11847, 2055) <class 'scipy.sparse.coo.coo_matrix'>\n",
      "year (11847, 2056) <class 'scipy.sparse.coo.coo_matrix'>\n"
     ]
    }
   ],
   "source": [
    "doc_hash_test = preprocess_new_feature(df_new_test,sample_size=11847)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1585 2051 nan\n",
      "1585 2053 nan\n",
      "1585 2054 nan\n",
      "1585 2055 nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-107-e7a3b57e516a>:8: DeprecationWarning: Assigning the 'data' attribute is an inherently unsafe operation and will be removed in the future.\n",
      "  doc_hash_test_np.data = np.nan_to_num(doc_hash_test_np.data)\n"
     ]
    }
   ],
   "source": [
    "doc_hash_test_np = doc_hash_test.toarray()\n",
    "\n",
    "for idx,i in enumerate(doc_hash_test_np):\n",
    "    for idxx,j in enumerate(i):\n",
    "        if np.isnan(j):\n",
    "            print(idx,idxx,j)\n",
    "            \n",
    "doc_hash_test_np.data = np.nan_to_num(doc_hash_test_np.data)\n",
    "\n",
    "for idx,i in enumerate(doc_hash_test_np):\n",
    "    for idxx,j in enumerate(i):\n",
    "        if np.isnan(j):\n",
    "            print(idx,idxx,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.45, 0.55],\n",
       "       [0.53, 0.47],\n",
       "       [0.59, 0.41],\n",
       "       ...,\n",
       "       [0.5 , 0.5 ],\n",
       "       [0.5 , 0.5 ],\n",
       "       [0.58, 0.42]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = clf.predict_proba(doc_hash_test_np) # predict the proba of -1 and 1\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = [round(x[1],1) for x in pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_all_pred (11847, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Popularity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27643</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27644</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27645</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27646</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27647</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Id  Popularity\n",
       "0  27643         0.6\n",
       "1  27644         0.5\n",
       "2  27645         0.4\n",
       "3  27646         0.6\n",
       "4  27647         0.5"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_pred = pd.concat([df_new_test['Id'],pd.DataFrame(pred)], axis=1).rename(columns={0: 'Popularity'})\n",
    "print(f'df_all_pred {df_all_pred.shape}')\n",
    "df_all_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_pred.to_csv('./data/comp1/pred.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "body (1000, 1024) <class 'scipy.sparse.csr.csr_matrix'>\n",
      "topic (1000, 1041) <class 'scipy.sparse.coo.coo_matrix'>\n",
      "author (1000, 1042) <class 'scipy.sparse.coo.coo_matrix'>\n",
      "img num (1000, 1043) <class 'scipy.sparse.coo.coo_matrix'>\n",
      "video num (1000, 1044) <class 'scipy.sparse.coo.coo_matrix'>\n",
      "week of day (1000, 1045) <class 'scipy.sparse.coo.coo_matrix'>\n",
      "weekend (1000, 1046) <class 'scipy.sparse.coo.coo_matrix'>\n",
      "month (1000, 1047) <class 'scipy.sparse.coo.coo_matrix'>\n",
      "hour (1000, 1048) <class 'scipy.sparse.coo.coo_matrix'>\n",
      "year (1000, 1049) <class 'scipy.sparse.coo.coo_matrix'>\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import hstack, vstack\n",
    "import scipy.sparse\n",
    "sample_size = 1000 #27643 #100\n",
    "\n",
    "## body\n",
    "doc_body = df_new['body'].iloc[:sample_size]\n",
    "hashvec = HashingVectorizer(n_features=2**10,\n",
    "                            preprocessor=preprocessor,\n",
    "                            tokenizer=tokenizer_stem_nostop)\n",
    "doc_hash = hashvec.transform(doc_body)\n",
    "print(\"body\",doc_hash.shape,type(doc_hash))\n",
    "\n",
    "## topic\n",
    "###########\n",
    "# tmp = df_new['topic'].iloc[:sample_size]\n",
    "# doc_topic = []\n",
    "# for i in tmp:\n",
    "#     doc_topic.append(i.replace('[', '').replace(']', '').replace(',', '').replace('\\'', ''))\n",
    "# hashvec = HashingVectorizer(n_features=2**10,\n",
    "#                             preprocessor=preprocessor,\n",
    "#                             tokenizer=tokenizer_stem_nostop)\n",
    "# doc_hash = hstack([doc_hash,hashvec.transform(doc_topic)])\n",
    "###########\n",
    "###########\n",
    "tmp = df_new['topic'].iloc[:sample_size]\n",
    "topic_set = set()\n",
    "\n",
    "for i in tmp:\n",
    "    #print(i)\n",
    "    items = i.replace('[', '#').replace(']', '#').replace(',', '#').replace('\\\"', '#').replace('\\'', '@').replace(' @', '')\n",
    "    items = items.split('#')\n",
    "    item_tmp = []\n",
    "    for item in items:\n",
    "        item_tmp.append(item.replace('@s', '$').replace('@', '').replace('$', '\\'s'))\n",
    "    #print(item_tmp)\n",
    "    topic_set.update(item_tmp)\n",
    "\n",
    "topic_list = list(topic_set)[1:] # [0] is empty: ''\n",
    "\n",
    "doc_topic = []\n",
    "for i in tmp:\n",
    "    items = i.replace('[', '#').replace(']', '#').replace(',', '#').replace('\\\"', '#').replace('\\'', '@').replace(' @', '')\n",
    "    items = items.split('#')\n",
    "    item_tmp = []\n",
    "    for item in items:\n",
    "        item_tmp.append(item.replace('@s', '$').replace('@', '').replace('$', '\\'s'))\n",
    "    topic_ids = []\n",
    "    for tag in item_tmp:\n",
    "        if tag != '':\n",
    "            topic_ids.append(topic_list.index(tag))\n",
    "    topic_ids.sort()\n",
    "    doc_topic.append(topic_ids)\n",
    "\n",
    "# print(topic_list)\n",
    "pad = len(max(doc_topic, key=len))\n",
    "doc_topic = np.array([i + [0]*(pad-len(i)) for i in doc_topic])\n",
    "doc_hash = hstack([doc_hash,doc_topic])\n",
    "###########\n",
    "print(\"topic\",doc_hash.shape,type(doc_hash))\n",
    "\n",
    "# ## time\n",
    "# doc_time = df_new['time'].iloc[:100]\n",
    "# hashvec = HashingVectorizer(n_features=2**10,\n",
    "#                             preprocessor=preprocessor,\n",
    "#                             tokenizer=tokenizer_stem_nostop)\n",
    "# doc_hash = hstack([doc_hash,hashvec.transform(doc_time)])\n",
    "# print(doc_hash.shape,type(doc_hash))\n",
    "\n",
    "## author\n",
    "tmp = df_new['author'].replace(np.nan, 'x').iloc[:sample_size]\n",
    "unique = list(set(tmp))\n",
    "doc_author = []\n",
    "for i in tmp:\n",
    "    doc_author.append(unique.index(i))\n",
    "# print(doc_author)\n",
    "doc_author = np.array(doc_author)\n",
    "doc_hash = hstack([doc_hash,doc_author.reshape((sample_size,1))])\n",
    "print(\"author\",doc_hash.shape,type(doc_hash))\n",
    "\n",
    "## img\n",
    "doc_img = df_new['image num'].iloc[:sample_size].to_numpy()\n",
    "doc_hash =hstack([doc_hash,doc_img.reshape((sample_size,1))])\n",
    "print(\"img num\",doc_hash.shape,type(doc_hash))\n",
    "\n",
    "## video\n",
    "doc_video = df_new['video num'].iloc[:sample_size].to_numpy()\n",
    "doc_hash = hstack([doc_hash,doc_video.reshape((sample_size,1))])\n",
    "print(\"video num\",doc_hash.shape,type(doc_hash))\n",
    "\n",
    "## week of day\n",
    "doc_w = df_new['week of day'].iloc[:sample_size].to_numpy()\n",
    "doc_hash = hstack([doc_hash,doc_w.reshape((sample_size,1))])\n",
    "print(\"week of day\",doc_hash.shape,type(doc_hash))\n",
    "\n",
    "## weekend\n",
    "doc_w = df_new['week of day'].iloc[:sample_size].to_numpy()\n",
    "doc_weekend = []\n",
    "for i in doc_w:\n",
    "    if i > 5:\n",
    "        doc_weekend.append(1)\n",
    "    else:\n",
    "        doc_weekend.append(0)\n",
    "doc_weekend = np.array(doc_weekend)\n",
    "doc_hash = hstack([doc_hash,doc_weekend.reshape((sample_size,1))])\n",
    "print(\"weekend\",doc_hash.shape,type(doc_hash))\n",
    "\n",
    "## month\n",
    "doc_m = df_new['month'].iloc[:sample_size].to_numpy()\n",
    "doc_hash = hstack([doc_hash,doc_m.reshape((sample_size,1))])\n",
    "print(\"month\",doc_hash.shape,type(doc_hash))\n",
    "\n",
    "## hour\n",
    "doc_h = df_new['hour'].iloc[:sample_size].to_numpy()\n",
    "doc_hash = hstack([doc_hash,doc_h.reshape((sample_size,1))])\n",
    "print(\"hour\",doc_hash.shape,type(doc_hash))\n",
    "\n",
    "## year\n",
    "doc_y = df_new['year'].iloc[:sample_size].to_numpy()\n",
    "doc_hash = hstack([doc_hash,doc_y.reshape((sample_size,1))])\n",
    "print(\"year\",doc_hash.shape,type(doc_hash))\n",
    "\n",
    "# print(doc_hash.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "doc_y_label = df_new['Popularity'].iloc[:sample_size].to_numpy()\n",
    "print(doc_y_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.580 (+/-0.052)\n"
     ]
    }
   ],
   "source": [
    "import xgboost\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "scores = cross_val_score(estimator=RandomForestRegressor(),\n",
    "                         X=doc_hash, y=doc_y_label,\n",
    "                         cv=10, scoring='roc_auc')\n",
    "print('%.3f (+/-%.3f)' % ( scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.3\n"
     ]
    }
   ],
   "source": [
    "# check xgboost version\n",
    "import xgboost as xgb\n",
    "print(xgboost.__version__)\n",
    "model = xgb.XGBRegressor()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
