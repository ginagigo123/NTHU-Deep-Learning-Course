{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6897c1b",
   "metadata": {},
   "source": [
    "# Convolution Neural Network\n",
    "\n",
    "In this lab, we introduce two datasets, **MNIST** and **CIFAR**, then we will talk about how to implement CNN models for these two datasets using tensorflow. The major difference between mnist and cifar is their size. Due to the limit of memory size and time issue, we offer a guide to illustrate typical **input pipeline** of tensorflow. Let's dive into tensorflow!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87af1d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import utils, datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4565715a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739e4840",
   "metadata": {},
   "source": [
    "## MNIST\n",
    "We start from a simple dataset. MNIST is a simple computer vision dataset. It consists of images of handwritten digits like:\n",
    "\n",
    "![](https://nthu-datalab.github.io/ml/labs/12-1_CNN/imgsrc/MNIST.png)\n",
    "\n",
    "It also includes labels for each image, telling us which digit it is. For example, the labels for the above images are 5, 0, 4, and 1. Each image is 28 pixels by 28 pixels. We can interpret this as a big array of numbers:\n",
    "\n",
    "![](https://nthu-datalab.github.io/ml/labs/12-1_CNN/imgsrc/MNIST2.png)\n",
    "\n",
    "The MNIST data is hosted on [Yann LeCun's](http://yann.lecun.com/exdb/mnist/) website. We can directly import MNIST dataset from Tensorflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bcfdd8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 3s 0us/step\n",
      "shape of train_images: (60000, 28, 28)\n",
      "shape of train_labels: (60000,)\n"
     ]
    }
   ],
   "source": [
    "# Download and prepare the MNIST dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()\n",
    "\n",
    "# Normalize pixel values to be between 0 and 1\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
    "print('shape of train_images:', train_images.shape)\n",
    "print('shape of train_labels:', train_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b708da7",
   "metadata": {},
   "source": [
    "## Softmax Regression on MNIST\n",
    "Before jumping to *Convolutional Neural Network* model, we're going to start with a very simple model with a single layer and softmax regression.\n",
    "\n",
    "We know that every image in MNIST is a handwritten digit between zero and nine. So there are only ten possible digits that a given image can be. We want to give the probability of the input image for being each digit. That is, input an image, the model outputs a ten-dimension vector.\n",
    "\n",
    "This is a classic case where a softmax regression is a natural, simple model. If you want to assign probabilities to an object being one of several different things, softmax is the thing to do.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83db138d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "# flating the training data for dense layers\n",
    "train_images_1 = train_images.reshape((60000, -1))\n",
    "test_images_1 = test_images.reshape((10000, -1))\n",
    "print(train_images_1.shape)\n",
    "print(test_images_1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9503170e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 10)                7850      \n",
      "=================================================================\n",
      "Total params: 7,850\n",
      "Trainable params: 7,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1 = models.Sequential()\n",
    "model_1.add(layers.Dense(10, activation='softmax',input_shape=(784,)))\n",
    "model_1.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "393177a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.4692 - accuracy: 0.8762\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3031 - accuracy: 0.9151\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2834 - accuracy: 0.9211\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2729 - accuracy: 0.9238\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2667 - accuracy: 0.9249\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x17d9fecc940>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile the model and train it for 5 epochs\n",
    "model_1.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_1.fit(train_images_1, train_labels, epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36166534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy : 0.9276\n"
     ]
    }
   ],
   "source": [
    "_, test_acc_1 = model_1.evaluate(test_images_1, test_labels, verbose=0)\n",
    "print('Testing Accuracy : %.4f'%test_acc_1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8485e4a0",
   "metadata": {},
   "source": [
    "From the above result, we got about 92.6% accuracy for Softmax Regression on MNIST. In fact, it's not so good. This is because we're using a very simple model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5c49b2",
   "metadata": {},
   "source": [
    "## Multilayer Convolutional Network on MNIST\n",
    "\n",
    "We're now jumping from a very simple model to something moderately sophisticated: a small *Convolutional Neural Network.* This will get us to over 99% accuracy, not state of the art, but respectable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "519a6e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n",
      "(10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# reshaping the training data to 3 dimensions\n",
    "train_images_2 = train_images.reshape((60000, 28, 28, 1))\n",
    "test_images_2 = test_images.reshape((10000, 28, 28, 1))\n",
    "print(train_images_2.shape)\n",
    "print(test_images_2.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61d01e7",
   "metadata": {},
   "source": [
    "## Create the convolutional base\n",
    "As input, a CNN takes tensors of shape (image_height, image_width, color_channels), ignoring the batch size. If you are new to color channels, MNIST has one (because the images are grayscale), whereas a color image has three (R,G,B). In this example, we will configure our CNN to process inputs of shape (28, 28, 1), which is the format of MNIST images. We do this by passing the argument **input_shape** to our first layer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31110305",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The 6 lines of code below define the convolutional base using a common pattern: a stack of Conv2D and MaxPooling2D layers.\n",
    "model_2 = models.Sequential()\n",
    "model_2.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model_2.add(layers.MaxPooling2D((2, 2)))\n",
    "model_2.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model_2.add(layers.MaxPooling2D((2, 2)))\n",
    "model_2.add(layers.Conv2D(64, (3, 3), activation='relu'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00d5d38",
   "metadata": {},
   "source": [
    "Let's display the architecture of our model so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b30c1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 3, 3, 64)          36928     \n",
      "=================================================================\n",
      "Total params: 55,744\n",
      "Trainable params: 55,744\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_2.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878882c0",
   "metadata": {},
   "source": [
    "Above, you can see that the output of every Conv2D and MaxPooling2D layer is a 3D tensor of shape (height, width, channels). The width and height dimensions tend to shrink as we go deeper in the network. The number of output channels for each Conv2D layer is controlled by the first argument (e.g., 32 or 64). Typically, as the width and height shrink, we can afford (computationally) to add more output channels in each Conv2D layer.\n",
    "\n",
    "### Add Dense layers on top\n",
    "To complete our model, we will feed the last output tensor from the convolutional base (of shape (3, 3, 64)) into one or more Dense layers to perform classification. Dense layers take vectors as input (which are 1D), while the current output is a 3D tensor. First, we will **flatten** (or unroll) the 3D output to 1D, then **add one or more Dense layers on top.** MNIST has 10 output classes, so we use a final Dense layer with 10 outputs and a softmax activation.\n",
    "\n",
    "To reduce overfitting, we will apply [dropout](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf) before the readout layer. \n",
    "\n",
    "The idea behind dropout is to train an ensemble of model instead of a single model. During training, we drop out neurons with probability \n",
    "$p$\n",
    ", i.e., the probability to keep is \n",
    "$1\n",
    "−\n",
    "p$\n",
    ". When a neuron is dropped, its output is set to zero. These dropped neurons do not contribute to the training phase in forward pass and backward pass. For each training phase, we train the network slightly different from the previous one. It's just like we train different networks in each training phrase. However, during testing phase, we don't drop any neuron, and thus, implement dropout is kind of like doing ensemble. Also, randomly drop units in training phase can prevent units from co-adapting too much. Thus, dropout is a powerful regularization techique to deal with overfitting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ac6b1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.add(layers.Flatten())\n",
    "model_2.add(layers.Dense(64, activation='relu'))\n",
    "model_2.add(layers.Dropout(0.5))\n",
    "model_2.add(layers.Dense(10, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2752f1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 3, 3, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                36928     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 93,322\n",
      "Trainable params: 93,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffa3dfe",
   "metadata": {},
   "source": [
    "As you can see, our (3, 3, 64) outputs were flattened into vectors of shape (576) before going through two Dense layers.\n",
    "\n",
    "### Compile and train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7aee3621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2751 - accuracy: 0.9165\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0938 - accuracy: 0.9740\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0658 - accuracy: 0.9819\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0519 - accuracy: 0.9852\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0447 - accuracy: 0.9873\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x17dbd0c36a0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_2.fit(train_images_2, train_labels, epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "467562ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy : 0.9899\n"
     ]
    }
   ],
   "source": [
    "_, test_acc_2 = model_2.evaluate(test_images_2, test_labels, verbose=0)\n",
    "print('Testing Accuracy : %.4f'%test_acc_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7e2032",
   "metadata": {},
   "source": [
    "As you can see, our simple CNN has achieved a test accuracy of over 99%. Not bad for a few lines of code! For another style of writing a CNN (using the Keras Subclassing API and a GradientTape) head here.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70edb729",
   "metadata": {},
   "source": [
    "# Cifar-10\n",
    "Actually MNIST is a easy dataset for the beginner. To demonstrate the power of Neural Networks, we need a larger dataset CIFAR-10.\n",
    "\n",
    "[CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) consists of 60000 32x32 color images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. Here are the classes in the dataset, as well as 10 random images from each:\n",
    "\n",
    "![](https://nthu-datalab.github.io/ml/labs/12-1_CNN/imgsrc/CIFAR10.png)\n",
    "\n",
    "\n",
    "Before jumping to a complicated neural network model, we're going to start with KNN and SVM. The motivation here is to compare neural network model with traditional classifiers, and highlight the performance of neural network model.\n",
    "\n",
    "tf.keras.datasets offers convenient facilities that automatically access some well-known datasets. Let's load the CIFAR-10 in tf.keras.datasets:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ac1c8f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (50000, 32, 32, 3)\n",
      "Y_train shape: (50000, 10)\n",
      "X_test shape: (10000, 32, 32, 3)\n",
      "Y_test shape: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Loading Data\n",
    "import math\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = datasets.cifar10.load_data()\n",
    "\n",
    "# normalize inputs from 0-255 to 0.0-1.0\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# convert class vectors to binary vectors\n",
    "Y_train = utils.to_categorical(y_train)\n",
    "Y_test = utils.to_categorical(y_test)\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('Y_train shape:', Y_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "print('Y_test shape:', Y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e669ae7",
   "metadata": {},
   "source": [
    "For simplicity, we also convert the images into the grayscale. We use the [Luma coding](https://en.wikipedia.org/wiki/Grayscale#Luma_coding_in_video_systems) that is common in video systems:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c5106b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x144 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# transform a 3-channel image into one channel\n",
    "def grayscale(data, dtype='float32'):\n",
    "    # luma coding weighted average in video systems\n",
    "    r = np.asarray(.3, dtype=dtype)\n",
    "    g = np.asarray(.59, dtype=dtype)\n",
    "    b = np.asarray(.11, dtype=dtype)\n",
    "    rst = r * data[:, :, :, 0] + g * data[:, :, :, 1] + b * data[:, :, :, 2]\n",
    "    # add channel dimension\n",
    "    rst = np.expand_dims(rst, axis=3)\n",
    "    return rst\n",
    "\n",
    "X_train_gray = grayscale(X_train)\n",
    "X_test_gray = grayscale(X_test)\n",
    "\n",
    "# plot a randomly chosen image\n",
    "img = round(np.random.rand() * X_train.shape[0])\n",
    "plt.figure(figsize=(4, 2))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(X_train[img], interpolation='none')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(\n",
    "    X_train_gray[img, :, :, 0], cmap=plt.get_cmap('gray'), interpolation='none')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156f23f6",
   "metadata": {},
   "source": [
    "As we can see, the objects in grayscale images can still be recognizable.\n",
    "\n",
    "## Feature Selection\n",
    "When coming to object detection, HOG (histogram of oriented gradients) is often extracted as a feature for classification. It first calculates the gradients of each image patch using sobel filter, then use the magnitudes and orientations of derived gradients to form a histogram per patch (a vector). After normalizing these histograms, it concatenates them into one HOG feature. For more details, read this [tutorial](https://www.learnopencv.com/histogram-of-oriented-gradients/).\n",
    "\n",
    "> Note. one can directly feed the original images for classification; however, it will take lots of time to train and get worse performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0a4c8901",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHOGfeat(image,\n",
    "               stride=8,\n",
    "               orientations=8,\n",
    "               pixels_per_cell=(8, 8),\n",
    "               cells_per_block=(2, 2)):\n",
    "    cx, cy = pixels_per_cell\n",
    "    bx, by = cells_per_block\n",
    "    sx, sy, sz = image.shape\n",
    "    n_cellsx = int(np.floor(sx // cx))  # number of cells in x\n",
    "    n_cellsy = int(np.floor(sy // cy))  # number of cells in y\n",
    "    n_blocksx = (n_cellsx - bx) + 1\n",
    "    n_blocksy = (n_cellsy - by) + 1\n",
    "    gx = np.zeros((sx, sy), dtype=np.double)\n",
    "    gy = np.zeros((sx, sy), dtype=np.double)\n",
    "    eps = 1e-5\n",
    "    grad = np.zeros((sx, sy, 2), dtype=np.double)\n",
    "    for i in range(1, sx - 1):\n",
    "        for j in range(1, sy - 1):\n",
    "            gx[i, j] = image[i, j - 1] - image[i, j + 1]\n",
    "            gy[i, j] = image[i + 1, j] - image[i - 1, j]\n",
    "            grad[i, j, 0] = np.arctan(gy[i, j] / (gx[i, j] + eps)) * 180 / math.pi\n",
    "            if gx[i, j] < 0:\n",
    "                grad[i, j, 0] += 180\n",
    "            grad[i, j, 0] = (grad[i, j, 0] + 360) % 360\n",
    "            grad[i, j, 1] = np.sqrt(gy[i, j] ** 2 + gx[i, j] ** 2)\n",
    "    normalised_blocks = np.zeros((n_blocksy, n_blocksx, by * bx * orientations))\n",
    "    for y in range(n_blocksy):\n",
    "        for x in range(n_blocksx):\n",
    "            block = grad[y * stride:y * stride + 16, x * stride:x * stride + 16]\n",
    "            hist_block = np.zeros(32, dtype=np.double)\n",
    "            eps = 1e-5\n",
    "            for k in range(by):\n",
    "                for m in range(bx):\n",
    "                    cell = block[k * 8:(k + 1) * 8, m * 8:(m + 1) * 8]\n",
    "                    hist_cell = np.zeros(8, dtype=np.double)\n",
    "                    for i in range(cy):\n",
    "                        for j in range(cx):\n",
    "                            n = int(cell[i, j, 0] / 45)\n",
    "                            hist_cell[n] += cell[i, j, 1]\n",
    "                    hist_block[(k * bx + m) * orientations:(k * bx + m + 1) * orientations] = hist_cell[:]\n",
    "            normalised_blocks[y, x, :] = hist_block / np.sqrt(\n",
    "                hist_block.sum() ** 2 + eps)\n",
    "    return normalised_blocks.ravel()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a70a1cd",
   "metadata": {},
   "source": [
    "Once we have our *getHOGfeat* function, we then get the HOG features of all images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5804a80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This will take some minutes.\n"
     ]
    }
   ],
   "source": [
    "X_train_hog = []\n",
    "X_test_hog = []\n",
    "\n",
    "print('This will take some minutes.')\n",
    "\n",
    "for img in X_train_gray:\n",
    "    img_hog = getHOGfeat(img)\n",
    "    X_train_hog.append(img_hog)\n",
    "\n",
    "for img in X_test_gray:\n",
    "    img_hog = getHOGfeat(img)\n",
    "    X_test_hog.append(img_hog)\n",
    "\n",
    "X_train_hog_array = np.asarray(X_train_hog)\n",
    "X_test_hog_array = np.asarray(X_test_hog)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e192d3",
   "metadata": {},
   "source": [
    "## K Nearest Neighbors (KNN) on CIFAR-10\n",
    "scikit-learn provides off-the-shelf libraries for classification. For KNN and SVM classifiers, we can just import from scikit-learn to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e9057b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# p=2 and metric='minkowski' means the Euclidean Distance\n",
    "knn = KNeighborsClassifier(n_neighbors=11, p=2, metric='minkowski')\n",
    "\n",
    "knn.fit(X_train_hog_array, y_train.ravel())\n",
    "y_pred = knn.predict(X_test_hog_array)\n",
    "print('[KNN]')\n",
    "print('Misclassified samples: %d' % (y_test.ravel() != y_pred).sum())\n",
    "print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7050e24a",
   "metadata": {},
   "source": [
    "We can observe that the accuracy of KNN on CIFAR-10 is embarrassingly bad.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a118ab61",
   "metadata": {},
   "source": [
    "## Support Vector Machine (SVM) on CIFAR-10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb934f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "from sklearn.svm import SVC \n",
    "\n",
    "# C is the hyperparameter for the error penalty term\n",
    "# gamma is the hyperparameter for the rbf kernel\n",
    "svm_linear = SVC(kernel='linear', random_state=0, gamma=0.2, C=10.0)\n",
    "\n",
    "svm_linear.fit(X_train_hog_array, y_train.ravel())\n",
    "y_pred = svm_linear.predict(X_test_hog_array)\n",
    "print('[Linear SVC]')\n",
    "print('Misclassified samples: %d' % (y_test.ravel() != y_pred).sum())\n",
    "print('Accuracy: %.2f' % accuracy_score(y_test.ravel(), y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78aa935",
   "metadata": {},
   "source": [
    "## CNN on CIFAR-10\n",
    "By above, SVM is slightly better than KNN, but still poor. Next, we'll design a CNN model using tensorflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585e3bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The 6 lines of code below define the convolutional base using a common pattern: a stack of Conv2D and MaxPooling2D layers.\n",
    "model_3 = models.Sequential()\n",
    "model_3.add(layers.Conv2D(64, (5, 5), padding='same', activation='relu', input_shape=(32, 32, 3)))\n",
    "model_3.add(layers.MaxPool2D(pool_size=3,strides=2,padding='same'))\n",
    "model_3.add(layers.BatchNormalization())\n",
    "                \n",
    "model_3.add(layers.Conv2D(64, (5, 5), padding='same', activation='relu'))\n",
    "model_3.add(layers.MaxPool2D(pool_size=3,strides=2,padding='same'))\n",
    "model_3.add(layers.BatchNormalization())\n",
    "                \n",
    "model_3.add(layers.Flatten())\n",
    "model_3.add(layers.Dense(384, activation='relu'))\n",
    "model_3.add(layers.Dropout(0.5))\n",
    "model_3.add(layers.Dense(192, activation='relu'))\n",
    "model_3.add(layers.Dense(10, activation='softmax'))\n",
    "model_3.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_3.fit(X_train, y_train, epochs=200,validation_data=(X_test, y_test),verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7e665c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, test_acc_3 = model_3.evaluate(X_test, y_test, verbose=0)\n",
    "print('Testing Accuracy : %.4f'%test_acc_3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccc669d",
   "metadata": {},
   "source": [
    "Although Cifar10 is larger than Mnist, it's not large enough for the dataset you will meet in the following lessons. For large datasets, we can't feed all training data to the model due to the limit of memory size. Even if we can feed all data into the model, we still want the process of loading data is efficient. **Input pipeline** is the common way to solve these."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18eab08b",
   "metadata": {},
   "source": [
    "## Input Pipeline\n",
    "### Structure of an input pipeline\n",
    "A typical TensorFlow training input pipeline can be framed as an ETL process:\n",
    "\n",
    "1. Extract: Read data from memory (NumPy) or persistent storage -- either local (HDD or SSD) or remote (e.g. GCS or HDFS).\n",
    "2. Transform: Use CPU to parse and perform preprocessing operations on the data such as shuffling, batching, and domain specific transformations such as image decompression and augmentation, text vectorization, or video temporal sampling.\n",
    "3. Load: Load the transformed data onto the accelerator device(s) (e.g. GPU(s) or TPU(s)) that execute the machine learning model.\n",
    "This pattern effectively utilizes the CPU, while reserving the accelerator for the heavy lifting of training your model. In addition, viewing input pipelines as an ETL process provides a framework that facilitates the application of performance optimizations.\n",
    "\n",
    "### tf.data API\n",
    "To build a data input pipeline with **tf.data**, here are the steps that you can follow:\n",
    "\n",
    "1. Define data source and initialize your Dataset object\n",
    "2. Apply transformations on the dataset, following are some common useful  techniques\n",
    "    * map\n",
    "    * shuffle\n",
    "    * batch\n",
    "    * repeat\n",
    "    * prefetch\n",
    "3. Create iterator\n",
    "\n",
    "### Construct your Dataset\n",
    "To create an input pipeline, you must start with a data source. For example, to construct a **Dataset** from data in memory, you can use ```tf.data.Dataset.from_tensors()``` or ```tf.data.Dataset.from_tensor_slices()```. Alternatively, if your input data is stored in a file in the recommended TFRecord format, you can use ```tf.data.TFRecordDataset()```.\n",
    "\n",
    "Once you have a **Dataset** object, you can transform it into a new Dataset by chaining method calls on the **tf.data.Dataset** object. For example, you can apply per-element transformations such as **Dataset.map()**, and multi-element transformations such as **Dataset.batch()**. See the documentation for **tf.data.Dataset** for a complete list of transformations.\n",
    "\n",
    "Now suppose we have simple data sources:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a451c697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of samples\n",
    "n_samples = 200\n",
    "\n",
    "# an array with shape (n_samples, 5)\n",
    "raw_data_a = np.random.rand(n_samples, 5)\n",
    "# a list with length of n_samples from 0 to n_samples-1\n",
    "raw_data_b = np.arange(n_samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5d8892",
   "metadata": {},
   "source": [
    "We can create our tensorflow Dataset object with these two data using **tf.data.Dataset.from_tensor_slices**, which will automatically cut your data into slices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5da8ea34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorSliceDataset shapes: ((5,), ()), types: (tf.float64, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "# this tells the dataset that each row of raw_data_a is corresponding to each element of raw_data_b\n",
    "raw_dataset = tf.data.Dataset.from_tensor_slices((raw_data_a, raw_data_b))\n",
    "print(raw_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b174b1e",
   "metadata": {},
   "source": [
    "#### Apply transformations\n",
    "Next, according to your needs, you can preprocess your data in this step.\n",
    "\n",
    "**map**\n",
    "For example, **Dataset.map()** provide element-wise customized data preprocessing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "96ac3ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(one_row_a, one_b):\n",
    "    \"\"\"\n",
    "        Input: one slice of the dataset\n",
    "        Output: modified slice\n",
    "    \"\"\"\n",
    "    # Do some data preprocessing, you can also input filenames and load data in here\n",
    "    # Here, we transform each row of raw_data_a to its sum and mean\n",
    "    one_row_a = [tf.reduce_sum(one_row_a), tf.reduce_mean(one_row_a)]\n",
    "\n",
    "    return one_row_a, one_b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dc5415b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ParallelMapDataset shapes: ((2,), ()), types: (tf.float64, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "raw_dataset = raw_dataset.map(preprocess_function, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "print(raw_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4690d3",
   "metadata": {},
   "source": [
    "#### shuffle\n",
    "**Dataset.shuffle(buffer_size)** maintains a fixed-size buffer and chooses the next element uniformly at random from that buffer. This way, you can see your data coming with different order in different epoch. This can prevent your model overfit on the order of your training data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b67aba2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = raw_dataset.shuffle(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a087ddd9",
   "metadata": {},
   "source": [
    "#### batch\n",
    "Now our dataset is one example by one example. However, in reality, we usually want to read one batch at a time, thus we can call **Dataset.batch(batch_size)** to stack batch_size elements together.\n",
    "\n",
    "**Note:** Be careful that if you apply **Dataset.shuffle** after **Dataset.batch**, you'll get shuffled batch but data in a batch remains the same.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "476312c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.batch(2,drop_remainder=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc75c935",
   "metadata": {},
   "source": [
    "#### repeat\n",
    "Repeats this dataset count times.\n",
    "\n",
    "**Dataset.repeat(count)** allow you iterate over a dataset in multiple epochs. **count = None or -1** will let the dataset repeats indefinitely.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "34a3230d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.repeat(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05642430",
   "metadata": {},
   "source": [
    "If you would like to perform a custom computation (e.g. to collect statistics) at the end of each epoch then it's simplest to restart the dataset iteration on each epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ebd39fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch: 0, Batch size of this epoch: 7\n",
      "End of epoch: 1, Batch size of this epoch: 7\n",
      "End of epoch: 2, Batch size of this epoch: 7\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "cus_dataset = raw_dataset.batch(32)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    size = 0\n",
    "    for batch in cus_dataset:\n",
    "        size += 1\n",
    "    print(\"End of epoch: %d, Batch size of this epoch: %d\"%(epoch, size))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a636739",
   "metadata": {},
   "source": [
    "#### prefetch\n",
    "Creates a Dataset that prefetches elements from this dataset.\n",
    "\n",
    "**Dataset.prefetch(buffer_size)** allow you decouple the time when data is produced from the time when data is consumed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9ddbc949",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5190627b",
   "metadata": {},
   "source": [
    "#### Consume elements\n",
    "The **Dataset** object is a Python iterable. This makes it possible to consume its elements using a for loop:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aa67fc5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch  0 , b are  [11 15]\n",
      "Batch  1 , b are  [1 2]\n",
      "Batch  2 , b are  [8 0]\n",
      "Batch  3 , b are  [10  7]\n",
      "Batch  4 , b are  [22 16]\n",
      "Batch  5 , b are  [ 6 13]\n",
      "Batch  6 , b are  [21 28]\n",
      "Batch  7 , b are  [ 5 19]\n",
      "Batch  8 , b are  [26 18]\n"
     ]
    }
   ],
   "source": [
    "# Here, we print the first 8 batches.\n",
    "for i,elem in enumerate(dataset):\n",
    "    print(\"Batch \", i, \", b are \", elem[1].numpy())\n",
    "    if i==8:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21101aa",
   "metadata": {},
   "source": [
    "Or by explicitly creating a Python iterator using **iter** and consuming its elements using **next**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "634dbccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch  0 , b are  [11  3]\n",
      "Batch  1 , b are  [0 7]\n",
      "Batch  2 , b are  [15 16]\n",
      "Batch  3 , b are  [9 2]\n",
      "Batch  4 , b are  [21 23]\n",
      "Batch  5 , b are  [18 24]\n",
      "Batch  6 , b are  [27 26]\n",
      "Batch  7 , b are  [5 6]\n"
     ]
    }
   ],
   "source": [
    "# Here, we print the first 8 batches.\n",
    "it = iter(dataset)\n",
    "for i in range(8):\n",
    "    print(\"Batch \", i, \", b are \", next(it)[1].numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0453d129",
   "metadata": {},
   "source": [
    "#### repeat+batch / batch+repeat\n",
    "The Dataset.repeat transformation concatenates its arguments without signaling the end of one epoch and the beginning of the next epoch. Because of this a Dataset.batch applied after Dataset.repeat will yield batches that stradle epoch boundaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d2f0083a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this function to plot the size of each batch.\n",
    "def plot_batch_sizes(ds,title):\n",
    "    batch_sizes = [batch[1].shape[0] for batch in ds]\n",
    "    plt.bar(range(len(batch_sizes)), batch_sizes)\n",
    "    plt.xlabel('Batch number')\n",
    "    plt.ylabel('Batch size')\n",
    "    plt.title(title)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2f9e888c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the bar diagram of repeat+batch\n",
    "repeat_batch_ds = raw_dataset.repeat(3).batch(32)\n",
    "plot_batch_sizes(repeat_batch_ds,'repeat+batch')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1fa8ce",
   "metadata": {},
   "source": [
    "If you need clear epoch separation, put Dataset.batch before the repeat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9df8e1a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the bar diagram of batch+repeat\n",
    "batch_repeat_ds = raw_dataset.batch(32).repeat(3)\n",
    "plot_batch_sizes(batch_repeat_ds,'batch+repeat')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed36c8a5",
   "metadata": {},
   "source": [
    "#### shufflt+repeat / repeat+shufflt\n",
    "As with Dataset.batch the order relative to Dataset.repeat matters.\n",
    "\n",
    "Dataset.shuffle doesn't signal the end of an epoch until the shuffle buffer is empty. So a shuffle placed before a repeat will show every element of one epoch before moving to the next.\n",
    "\n",
    "But a repeat before a shuffle mixes the epoch boundaries together.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a634bedd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x17e615a13d0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEGCAYAAACKB4k+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAABk4ElEQVR4nO2dd3hb5dXAf0eSLe/EM3GmM5w9yU4IBAgQCCWsMsqmQFlldFEos0BLKaUtUMoMIeyvzLBCBpBAtuNsZ9iJndiJ995Der8/ruQ4sWXLtmR5vL/n0SPp3vfeeyzbOvdsUUqh0Wg0Gg2AydcCaDQajabzoJWCRqPRaOrRSkGj0Wg09WiloNFoNJp6tFLQaDQaTT0WXwvQHqKiolRcXJyvxdBoNJouxdatW/OUUtFN7evSSiEuLo6EhARfi6HRaDRdChE57Gqfdh9pNBqNph6tFDQajUZTj1YKGo1Go6nHa0pBRBaLSI6I7G6wbZKIbBSR7SKSICLTHdtFRJ4XkRQR2Skip3hLLo1Go9G4xpuWwhJgwUnbngEeV0pNAh5xvAc4D4h3PG4F/utFuTQajUbjAq8pBaXUWqDg5M1AmON1L+CY4/UiYKky2Aj0FpFYb8mm0Wg0mqbp6JTUe4FvReRZDIU027G9P5DeYF2GY1vmyScQkVsxrAkGDRrkTVk1Go2mx9HRgebbgfuUUgOB+4A3WnsCpdSrSqmpSqmp0dFN1l5oOhE/JeexL6vE12JoNBo36WilcD3wieP1/4DpjtdHgYEN1g1wbNN0YYorarllaQL/XpXsa1E0Go2bdLRSOAac7nh9JuD8tlgGXOfIQpoJFCulGrmONF2L/21Np7LWRn55ja9F0Wg0buK1mIKIvA/MA6JEJAN4FLgF+LeIWIAqHLEB4GvgfCAFqABu9JZcmo7BZlcs3WBU0hdqpaDRuE1JVS3vbDzMldMGERHs3+HX95pSUEpd5WLXlCbWKuBOb8mi6Xh+2J/DkYIKYkKtFFZopaDRuENpVS3XL97MtiNF7Egv4uVrpiAiHSqDrmjWeIW3NhymT5iVi0/pT2FFLXa7ngWu0TRHWXUd1y/ezL6MfO4fcpBVe47x5c6O96J36S6pms7Jwdwy1h7I5TdnjyDEasFmV5RU1dI7qONNYY2mK1BWXccNizeTnJHN2oGLic78kZro+3jk8wBmDYskKsTaYbJoS0Hjcd7ecBg/s3DV9OM+0QIdV9BomqS8uo6b3tzCofQM1vb9F9E568AviBv7plJebeORz3e3fBIPopWCxqOUVdfx0dYMFo6PJTrUSrhDKei4gkbTmIqaOm5csoWjR1JYE/k3wov3weVLYdRCwo6t4975w/l6VxZfdaAbSSsFjUf5JDHD8I3OjgMgIshpKdT6UCqNpvNRWWPjpiVbyD+8m5W9niK0Ohuu+RhG/wyGzoPyXG4dWcWEAb14+PPd5JdVd4hcWiloPIZSirfWpzFxQC8mDwoHIDzYD9Bpqb6muKKW29/ZyqZD+b4WRYOhEH751hYq0xL4OuQpgqQWbvgShsw1Fgwxyrksh9fy98smUlZVxyPL9nSIbFopaDzGTyl5HMwt57pZcfXb6mMK2n3kU15ee5Bvdmdx89IEUnJKfS1Oj6aq1sYtSxMwp/7AR4F/wRoUCr9cAf0mHV/UeyBEDodDaxjZN5R75sfz1c5Mvt7lfTeSVgoaj/HW+sNEBvtzwcTjDW4D/cxYLSZtKfiQnNIq3lyXymkjorFazNzw5hZySzvGFaFpzHubjhB66CuWBDyLX2Qc3LQCIoc1Xjh0HqT9BLZafnXaUMb378XDn+32etKGVgoaj5BeUMHqfdlcNX0QVou5fruIEBHsr1td+JAXv0uhzqb484VjeeP6qeSVVXPz0gQqa2y+Fq1HUrLnW/7j/zzm/lPgxq8hzMWUgKHzoLYcMhKwmE38/ecTKKmq9Xo2klYKGo/w9sbDmES4embjduYRwf7aUvAR6QUVvL/5CJdPG0hcVDATB/bm31dOZmdGEb/5v+26qLCDUUoxJmsZ5ebecO2nEBjuenHcqSAmOPQDAKP6hnH3mfF8uTOTb7zoRtJKQdNuKmtsfLglnXPH9iG2V2Cj/RHB/jqm4CP+tSoZEeHuM+Prt507ti9/On803+zO4unl+3woXc/jSG4RM+2JZMfOA/+g5hcHhkO/yfVKAeC2ecMY2y+Mhz/3nhtJKwVNu/l8+1GKK2u5vkGAuSHhQdpS8AXJ2aV8ui2D62cNpm+vgBP2/fLUIVw3azCvrj3EOxsP+0jCnkd64grCpJKAcT9z74Ch8yBjC1QZM0n8zCae/flEiipqeW7lfq/IqJWCpl0opViyPo1RfUOZPiSiyTURwf66otkH/GPFAYL8Ldw+b3ijfSLCIxeM4cxRMTzy+W6+35/jAwl7Hpbkb6hQVmInn+feAUPngbLB4fX1m0bHhvHS1afwu3NGekVGrRQ07WJLWiH7skq5fnacy26O4UH+lFTVUWuzd7B0PZcd6UUs35PFzXOHuGy/bDGbeOGqyYyODeOudxNJOqYn5HkVpRhW8CNJQVMxW1twHTkZMB0sgSe4kADOGdvXa73EtFLQtIu31qfRK9CPiyb1d7kmwlHAVlShq5o7imdX7Cc8yI9fnjqk2XXBVguLb5hGWKAfNy3ZQlZxVQdJ2PMoTUsgWuVROOAs9w/yC4DBsxopBW/iNaUgIotFJEdEdp+0/dcisk9E9ojIMw22PyAiKSKyX0TO9ZZcGs/x5c5jLN+TxRXTBhLob3a5Tvc/6lg2HMznx+Q87pg3nNAAvxbX9wkLYPEN0yioqOGVtQc7QMKeScHWz7ApIWziBa07cMjpkLsXSrO8I9hJeNNSWAIsaLhBRM4AFgETlVJjgWcd28cAVwJjHce8JCKuv2U0PmfphjR+/f42Jg/szZ1nNPZZN8TZ/yi/TCsFb6OU4tkV++kbFsC1swa7fdzo2DD6hgXohAAvEnRoOYlqBGPjmyhUa46h84znQ2s8LlNTeE0pKKXWAgUnbb4deFopVe1Y44xuLQI+UEpVK6VSMcZyTveWbJq2o5TiHyv288jnezhrVAxv/3IGvQKbvxuNCNGWQkfx3b4cth4u5O6z4gnwa919VZC/mXJd0OYdCg8TXZHCzuA5hFhbOcam7wQjPbWDXEgdHVMYAcwVkU0iskZEpjm29wfSG6zLcGxrhIjcKiIJIpKQm5vrZXE1Damz2Xnw01288F0Kl08dwMvXTGnWbeTkeKdUrRS8id2u+Pu3+xkcGcTPpw5o9fHBVgsVNXVekExj2/c1AKVx57T+YJPJcCEd+gGU94sNO1opWIAIYCbwe+D/pJUDSJVSryqlpiqlpkZHR3tDRk0TVNXauOPdRN7fnM6dZwzjb5dOwGJ278/HmSWhXRPe5ctdmezLKuU3Z4/Az83fTUOC/M2UV2tLwRtU7lpGsr0/Q0ZOaNsJhs6D0mOQl+xRuZqio5VCBvCJMtgM2IEo4CgwsMG6AY5tmk5AcWUt172xmRVJ2Tz6szH8/txRrRom7m8xEWq16KpmL1Jrs/Pciv2M6hvKzyb0a9M5gvzNuh+SN6gsJChzEyvtU5gyuJm2Fs1RH1f4wVNSuaSjlcJnwBkAIjIC8AfygGXAlSJiFZEhQDywuYNl0zRBdkkVV7yygW3phTx/1WRunNN8iqMrwnX/I6/y0dYM0vIr+O05IzGZWmV81xPsb6Fcu488T/JKTMrG1oBZ9O/duA2MW0QMgd6DO0QptDLi4T4i8j4wD4gSkQzgUWAxsNiRploDXK+UUsAeEfk/IAmoA+5USulbFh+TW1rNJS+tp6iihsU3TGNufNvddeHB/hToOgWvUFVr41+rDnDKoN7MHx3T5vMEWc1UaEvB8+z7ijzCCYyb3ioLuxFD58GeT8FWB2avfXV7Tykopa5ysesaF+ufAp7yljya1rNiWwqXlL7LuT//FePaoRAAIoL8yO2gcYI9jbfWp5FdUs3zV05u15dOsL+F8mptKXiUumrsyStZUTedU+Ii23euofMg8S3I3A4DpnpCuibRFc0a1+z5hN/6fcTYz86Bj2+BvJQ2n8pwH2lLwdMUV9by0g8HmTcymhlD2/elE+RvobrOTp1uR+I5Un/EVFvOSvvUtscTnDhGdHLo+/bL1QxaKWiaRClFaE4iZeYwZM49sO9L+M80+ORXkN/6qtdI3RTPK7yy5iDFlbX8/tz2N0cLthrpxRW12oXkMfZ/RY0pgETzeMb0C2vfuYIjjZoFLxexaaWgaZIjBRWMte2lMGIynP043LMTZt0JSZ/Di9Pg09uh4JDb5wsP9qey1qazWzxITkkVi9elsmhSP8aWrofnJ0PFyfWi7uOsOdG/Iw9ht8P+b9hqmcyoATFtShNuxNB5kL4Jasrbfy4XaKWgaZLt+w8xzJRJwNBZxoaQaDjnSbh3J8y8HfZ8Ai9Mhc/uhOqWB8E7C9h0VbPneP67ZOpsit+dGgXLfm0o6aydbT5fsL8RYtRxBQ+RuQ1KM/mkYiJT49rpOnIydB7YauDIBs+crwm0UtA0Sf7+dQBEjjz1xB0hMXDuU3DPDph+K2x/BxIWt3g+Z1M87ULyDGl55XywOZ2rpg9i4OYnoCLf2NGO4qYgh6WgM5A8xL6vUWJiVd2k9scTnAyaBWZ/r6amaqWgaRK/YwnYMGEaMKXpBaF94bynIaQv5LY8ASpCd0r1KP9YeQA/s4nfxh2CnR/Cab8H/5B2KYVgq7YUPMr+rzkWNolCwjhlkIeUgn8QDJyhlYKmY8kprWJY1R4KQke2PEc2eoRbSiFc9z/yGLuPFvPFjmPcMTOK3qv/ADFjYO7vICoe8rWl0CkoSIWcJNaapjE8JsSzA3GGzoOsXVCe57lzNkArBU0jth7KZaLpIGrgjJYXR40w7k5baNQVod1HHuPv3+6nd5Afv6p5E8qyYdF/wOIPkfGesRR0VXP72f8NAO8UjmOKp6wEJ0PPMJ5TvZOFpJWCphGH924hWKqJGDGn5cVRI6C62PhyaoZegX6I6KZ47WXDwXzWHMjlqQm5+O94B2bfDf1PMXZGjYDi9DZnpgT6aUvBY+z/murwkeypimSKp4LMTvpNAmsvSN/i2fM60EpB0wj7EaPtlCVuZsuLo0YYz3kHml1mNgnhQf66KV47UErxt+X7GBqmOD/1r4ZlMO+PxxdEOYYdtaGOBI5bChU6ptA+Kgrg8HpSIk4D8FyQ2YnJDHdsgAV/9ex5naf3ylk1XZbSqlr6le6kzC8Keg1s+QCnUnArruCnq5rbwYqkbLanF/Fyv6+Q4gxY9CL4NWiw5qaCdoUzpqAH7TTPX7/ey+s/NlOjk7wClI1V9imEB/kxNCrY80L06g/t6aPUDN7rqqTpkmw9XMhkSaaq7xRC3PmjC+vndtZLhK5qbjM2xwCdReFpjEh7D2bcDoNOsuQihgIC+W1rR2K1mDCbRA/aaYZjRZW8stZQCDU2O3fMa2IU7b4vIaQvy3L6MGVwaPua4PkAbSloTmDPgWQGm3IIjXcjngDG3UpUPOS5l4GkU1LbxseJGaTnFPAX8ytGC+WzHm68yC8Qeg9qs6UgInrQTgt8szsLgLnxUTyzfD+Lf0o9cUFNBSSvomrYAg7mVzJlcIQPpGwfblkKIjIbiGu4Xim11EsyaXxIxcGNAFiHzHL/oKiRkPZji8sigv3Znl7URsl6LsWVtTyzfB9Ph39BcNlhuG4Z+LtwSTizwdpIsL8eydkcX+/KZFTfUBbfMI273kvkz18mEeBn5hczBhkLDq6Gukr29J4HeCGe0AG0aCmIyNvAs8CpwDTHw3t9WzU+o7rORu+CbdSJH8ROdP/AqHgoOdpiu4vwYMNSUB0wZ7Y78ey3+xlQkcRFVZ/ClBth6OmuF0fFG+4je9s6nQb565kKrsgqrmLr4UIWjo/Fz2zihatO4YyR0fzps118vDXDWJS0DALDWVUxHD+zMGFAL98K3QbcsRSmAmOU/k/u9uzMKGYiBygNH0e4xer+gdGODp15ycfTI5sgIsifWpuitLqOsAC/dkrbM9iZUcQ7m9L4MfxDxNwHzv5z8wdExUNthaGke7uRKHASetCOa77ZnQnAeeNjAWPM7H+vmcIv39rC7z/aQYC5joUHlsPoC0lIL2Vsv14EONJ8uxLuxBR2A31be2IRWSwiOY4payfv+62IKBGJcrwXEXleRFJEZKeIuP5m0XiNhIPZTJRDBAx1IxW1IfVZL827LZz9j3StgnvY7Io/fbqbS4J2MaBiD8x7AAJaaL8cGW88t7GyOUgP2nHJ17syGdknlOExIfXbAvzMvHadMSvh4/+9B9Ul1I5cyI6MYqZ2QdcRNKMUROQLEVkGRAFJIvKtiCxzPtw49xJgQRPnHQicAxxpsPk8jLnM8cCtwH/d/xE0niIneQtWqSVwaCviCQDhQ0DMLQabI3VVc6t4b9Nhdh8t5NHgjyFiGEy6uuWD3FTQrgjW7qMmyS6pIuFwIec7rISGBPlbWHzDNK4M3U6ZCuSV9EHU1Nm7ZDwBmncfPdueEyul1opIXBO7/gn8Afi8wbZFwFKHi2qjiPQWkVilVGZ7ZNC4j82uCMhMAAEGTG/dwRZ/Ix2yhayXcN0Uz21yS6t55tv9/K7fLsIKkuGyxe7N5Q2JAWtYm5VCkNVCeUFFm47tzizfnYVScP74pp0moX7CfElgrf80nv3uMNA1g8zQjFJQSq0BEJEhQKZSqsrxPhDo05aLicgi4KhSasdJubv9gfQG7zMc2xopBRG5FcOaYNCgQW0RQ9ME+7NKGWvfT3lIP4LDGt8NtUjUCMhtXilE1DfF0wVsLfGXr/diq63h1roPoM94GHOxewfWpwi3LS012N9MhU5JbcRXuzKJjwkhvk9o0wuObMBUmc+Un11P/JoQzCYhJiygY4X0EO4Emv8HzG7w3ubYNq01FxKRIOBBDNdRm1FKvQq8CjB16lQd/PYQW9IKONt0AAY2k9nSHNEjjEpOWy2Ymw4ihwcb23VMoXk2HMzn021HeW3MbvwOHYYL/g9MrSgpiox3K0W4KYJ0Smojckqq2JJWwN1nxrtetHcZWAIIHbeAryYGUdmFR5q685dmUUrV/xc7XrelD+wwYAiwQ0TSgAFAooj0BY4CDVMlBji2aTqIA8n76CcFBLU2yOwkagTYa6EwzeWSEKsFP7OQr5WCS2rq7Dz8+W6GhZuYn/OW0Ts/vpX3UW6mCDeFMyVVJxse59s9huto4QQXFrTdDnu/hOHzwRqCv8VEr8Cum13njlLIFZELnW8cLqBWN/JWSu1SSsUopeKUUnEYLqJTlFJZwDLgOkcW0kygWMcTOg6lFOrIJgBkkBvtspsiypmW6tptIWI0xdOWgmte/+kQKTllvDJyO1KWCWc92voeN85gcxvaXQRbLdTZFTW2ttU5dEe+2pXJsOhg4htkHZ3AsUQoPQajf9axgnkJd5TCbcCDInJERI4A9+Pw6TeHiLwPbABGikiGiPyymeVfA4eAFOA14A435NJ4iCMFFQyr3kudKQD6jGvbSZwdOlvwZUcE606prsgorOD51cksGh3C8P2vwrCzIM7NdiMNiXK4OfJarxTqB+3ouAJgBPw3pxawcHys6x5GSZ+DyQIjGiVbdknciSnYlVIzRSQEQClV5gg+N4tS6qoW9sc1eK2AO92QReMFNqcWMMV0gJo+k7C4iAe0SEAvCI1tMdisLQXXPLYsCZMIT8asgdSCpvsbuUPEUBBTm4LNwf7HB+04s8V6Msv3ZGFXcL4r15FSsPcLGHI6BPbuUNm8hTuWwsdgKAOlVJlj20feE0nT0Ww7lMlYU1rr6xNOxo2sl4gQbSk0xcqkbFbtzeb+uVGEbnsFRl8I/Sa37WQWq9E0rw0FbEFWPWinId/symRodDAjXWUdZe+GwlQYc2HT+7sgLi0FERkFjAV6icglDXaFAV0z10rTJKWpCfhhg7bGE5xEjTSGyCvl0g8eoS2FRjiH58THhHCN7VOjTcWZD7XvpG1sjKfnNB8nr6yajYfyuWPecNeuo71fGFbZyIUdK5wXac59NBK4AOgNNIyglAK3eFEmTQeSU1pFv5Kd4AcMaFWWcWOiRkB1iTGaM7TpIp/wYH+KKmux2RVmU9fqM+8t9hwrISWnjOcWRGH+8TWYeNXxflJtJSremOFrt7cqnTXIX09fc/Kt03XURBVzPUnLYNBsCInuOMG8THPFa58Dn4vILKXUhg6USdOBJKQVcoopmaqwIQQER7XvZM4AZ+5+l0ohIsgPpYx20BHaZw3AFzuPYTEJCwveAWWH0+9v/0mj4qGuypjZHD7Y7cOOxxS0pfD1rkyGRAUzOtaF6ygvGXL3woK/daxgXsadW4htInKniLzkaHK3WEQWe10yTYew+VA+U0zJ+A1up+sIGnRLdR1XCK/vf1Td/ut1A+x2xZc7Mrk0rhrrrndh6o2t+hJ3ibMxXitdSMdjCj3YUshIIL8gn42HCjh/fN9mXEeOFnCjL+g42ToAd5TC2xhdUs8F1mAUlrW+KkbTKUk/tJcoKcbsCaUQGgv+oc0qhYhg3eqiIYlHCjlaVMmdpo+NAPHc33nmxPW1Cq1TCvWWQk9NST22DV4/C8vi+QxUxzhvXDOuo71fQP8p0GtAx8nXAbijFIYrpR4GypVSbwELAQ98g2h8TWlVLWF5icab1jbBawo3+u6EB+lOqQ1ZtuMYvS01DMxaaXRBDW1TW7HGBEdBQO9Wp6X2eEshcSlYAjBV5PGF9RHGViY0va4o3VAgo7tP1pETd5SC85auSETGAb2AGO+JpOkoth4uZLIkU2cJgZjRnjlpC43xIkN0p1QndTY7X+/K5I6Bh5G6Ks+mNdYr6Fa6j/x6cPZRTTns+ojqERdyQdUTVAbFIu9eButfNDLqGrL3C+O5m1QxN8QdpfCqiIQDD2O0o0gCuldkpbtSVQyl2U3uKiiv4T/fpzDVfAAZMAVMHpoQFT3CKPl30XdHWwrHWX8wn7yyGhb6JRp39YNmt3hMq2hDWqrFbMLfYqK8J1oKSZ9DdQnrws7jsD2avMu/gFEXwIo/wae3QW3V8bV7vzCq/yOH+U5eL9FiRbNS6nXHyzXAUO+Ko/EYdhssWQhZuyB6NAw7A4adCYNnszffxi1LEygrLWa0JR0ZfKXnrttwyEsTozkD/MwE+Zt1rQKG6yjcCv1y1sDI89ybl9AaIofD9nehqqTliW0N6LHtsxPfhohhLDnan4ER5YweHAuD3oIfn4XvnzJccVe+awyUOrIB5v3R1xJ7hRYtBRGJFJEXRCRRRLaKyL9EJLIjhNO0g10fGQph8rVGeuiWN+Ddy7A/PZjilxfwi5qP+HLuEQS7Z+IJTtxojBcepKuaq2ptfLs7i1/FZSNVRcYdqadpY7A5yN/S8yyFvGQ4sp7K8Vez/mA+5zt7HZlMcPof4Mr3jL/pV+fBD38FVLeMJ4B7vY8+ANYClzreXw18CMz3llCadlJXY9zZ9B0PP3seTCbs1RV88vlH5O9czjkBe7mj7l2jXSECA6Z47toRQ4zmYC1kIPV099GaA7mUVtfxM2siWAINK87TNGyM19/933GwtQdaColLwWThO+uZ1NmPsWDsSXU2oxbCzavg/atg65vGeFRPxeE6Ge4ohVil1BMN3j8pIld4SyCNB0h8C4oOw9UfgclEWXUd932YxMqkSC6bcj/XXzQOqvPh0A9GiX6gB8cGmv2Mmc25ruc1hwfrVhfLdhwjMsiPflmrDYXgH+T5i9TPzm5lBpK/hYouPCSm1dTVwI73YcQCvjhop0+YlYkDejdeFzMabvkOlj8AQ+e1vqV5F8EdpbBCRK4E/s/x/jLgW++JpGkXNeWw5hkYPAeGz+dwfjm3LE3gYG45j/5sDDfMjjPMYr8YmHC5d2SIHtlsgDMiyI/UvDKX+7s75dV1rN6bzT2jy5EDx+DMNnZDbQmLP4THtUEpmHtWm4sDy6E8l5qJ17LmvVwumzIAk6sWLEERcMkrHStfB9NcQ7xSQGGMcr8Xo4gNwAyUAR6qstF4lI3/hfIcuOIdNqUWcOvbWwFYetN05gxvZxsLd4mKN/7RXIzmNCyFnlu8tjIpm6paO4sCEg1LzZt9+KNGtHrYTpC/hcKKSi8J1AlJXAqh/Vhjm0Bl7TbOGeuhWpEuistAs1IqVCkV5ng2KaX8HA+TUsr9VAZNx1FRAOuehxHnwaAZPLpsD72D/Fh215yOUwhgBJvtdS5Hc0YG+1NWXUd1XQ9yUTRg2Y5j9OsVQGzmd4ZFFxThvYtFxUP+QSMbzU2CreaeU7xWnAEpq2DyNazYm0togIUZQ3p2Hk0rpoG3DkePpBwR2d1g299FZJ+I7BSRT0Wkd4N9D4hIiojsF5FzvSVXt2bdv4wupWc9TEF5DfuySrl86kAGRwZ3rBzOrBcXcQVn/6Oiip5nLRSW17D2QC7XjaxDcvcaAUxvEhUPtmooOuL2IUH+lp7T5mLbuwDUTbyaVXuzOWtUDP4Wr30tdgm8+dMvAU62i1cC45RSE4ADwAMAIjIGuBJjfsMC4CUR8VA1VQ+h5BhsesWIE/QZy+bUfABmDvXiXagr6rNemvZlR/TgArZvdmdRZ1csCtxhbPC6UmhQN+Imwf49xFKw22Db2zD0dBKKQymsqOWck7OOeiBeUwpKqbVAwUnbViilnH9tGzGa6wEsAj5QSlUrpVIxZjV7MHm+B7DmGeOPfN4DAGw8VECgn5nx/Xt3vCwBYUZzPBdKwWkp9MQMpGU7jjI0Opi+x1ZB3wnQe5B3L+jsltqKWoUgq4XKWht2u2p5cVfm0A9Ga/FTrmPFnmz8LSZOH9F95iK0FbeUgoiYRaSfiAxyPjxw7ZuAbxyv+wPpDfZlOLY1JcutIpIgIgm5ubkeEKMbkH/QCJZNucGoEwA2HMxnaly470zhqBGuLQWHUsjvYUohu6SKTakFXDnaH0nf7J2CtZMJjoTAiFZlIAX5m1EKqrp7zCdxKQSGo0Yu5Ns9WcwdHkWw1cNV5V0Qdyqafw1kY7h+vnI8vmzPRUXkT0Ad8G5rj1VKvaqUmqqUmhodrbU6YBSqWaxw2u8ByC+rZn92KTOH+jBg5uy7c3IjMY73P+ppTfG+3JmJUnBx0C5Aed915KSVjfGCHSM5u3VcoTwP9n0FE68iKbeao0WVPT7ryIk7avEeYKRSKt8TFxSRGzDGfJ6lVP03xlFgYINlAxzbNC2RuRN2fwxzf1vfdnlzquG187lSqC6B0iwIO7Enfe8gI021p8UUlu04xrj+YURnrDTqB/qM7ZgLR8XDgRVuL68fyVlTB1i9JJSP2fkh2Gth8rWs2JmNSeCs0VopgHvuo3Sg2BMXE5EFwB+AC5VSFQ12LQOuFBGriAwB4oHNnrhmt2f1n40Om7Pvrt+08VA+gX5mJgzo5Tu5op0BzsZuCz+zibAAS4+KKRzOL2dHehGXjAkzZiePuqDjKmIj443alcoit5YHW7u5paCU4ToaMA36jOHbPVlMHRxBVEg3VYCtpLnitd84Xh4CfhCRr4D6GYpKqeeaO7GIvA/MA6JEJAN4FCPbyAqsdIy426iUuk0ptUdE/g+jLXcdcKdSqpv+RXqQtHWQshLmPw6Bves3bzxUwNS4cPzMPkyti2qgFIae3mh3ZIiVgh6UkvrFjmMAXBS6F2w1Hec6ggaN8VJgwNQWl59oKXRDMrZA7j648AWO5FewL6uUhxZ2zz5GbaE595FzWvURx8Pf8XALpdRVTWx+o5n1TwFPuXt+DfDdE0aWz/Rb6zc54wkXTurnQ8FocTRneJBfj7IUlu04xrS4cCKOfAJBUTCwA4cXNlTQbiiF4Prpa930vizxLfAPgbGXsGJzFgDnjNGpqE5cKgWl1OMdKYimleQfNHq6n/PkCc3UOkU8AY5P/nJRwBYR7M+xoqom93U30gsqOJBdxp8XxsNPK2DMIs8NNXKH8MGOzrXuBZsD/bqxpVBVArs/gfGXgTWEFUm7GNU3lEGRXmhI2EVxJ/to5UmVx+Eiohvi+ZoDy43nk9IaO0U8wUkzjfHCg3pO++z1B/MAmB94wAi+d0QqakPMfhAx1O201G4dU9j/NdRWwORryS+rJiGtQBesnYQ7TudopVSR841SqhA9o9n37P/GmKjmqEtwsuFQvu/jCU6i4o3RnFUljXZFBBuDdlQTKavdjXUp+cSEWonNXA1+wU3GWLxOpPtpqd06ppC8EoJjoP9UVu/Nwa7gnDE666gh7nxz2BoWq4nIYIzuqRpfUVlkuI5GnthFJK+smgPZZcwa1kkaejmnsDVRTRse7E9Nnb37+q0dKKVYfzCfOUPDkf1fw/CzwC+w4wWJioeCQ2Br+Yu+3lLobr8buw0OrjZ+ByYTK5Ky6N87kLH9dH/PhrijFP4E/CQib4vIOxhT2B7wrliaZklZZXQhHXHeCZs7TTzBSX1jvMZui57S/yglp4y8smoWRmZCWRaM/plvBImKN/Lyiw63uDTAYkaE7jdT4dg2qCyE4fMpr65jbXIe54ztg3TTYTltpUWloJRaDpyCMYLzA2CKUkrHFHzJgeUQFNkok2TjoXyC/M2M798J4gnQ7GjO+v5H3byqef1Bo+ZzevUG47OIP9s3gjgVdPaeFpeaTEKQn7n7WXHJKwGBYWey9kAuNXV2nXXUBO46nm1ADlACjBGR07wnkqZZbLWQvALiz22UwbLxUD5T4yI6RzwBjABnzGjHwJ0T7zqd/Y+6u6Ww/mAeg8KthKUth7hTPTv6tDX0HQ9h/WH148Z0vhYI9Ld0P/dRyipjVnVQBCuSsgkP8mNanI9+H50Yd7KPbsZwGX0LPO54fsy7YmlccmQjVBW7jCf4pFV2c8z9HeQkwbalJ2yO6AGWgs2u2HgwnyeD/s8oHJtwpe+E8QuEi182Upm/fbDF5d1u0E5FARzdCvFnU2uzs3pvNmeN7oOls9xAdSLc+UTuAaYBh5VSZwCTgSJvCqVphgPLwexvDHtvQKeLJzgZs8iYLvbdkye0WTgeU+i+Vc17M0u4svZTTsv/EKb/Cib6UCkADDkNZv8ati4xmsE1Q7cbtHPwO0DB8PlsOlRASVWdzjpygTtKoUopVQUgIlal1D5gpHfF0rhk/zeGG8IaesLmThdPcCICC/5q3Kmt/Xv95tAAC2aTUFBe3czBXZu8H1/nAb/3qRp1MSx4uuN6HTXHmQ8ZcxyW/dpoVuiCbjdoJ2WV0UK832RWJGUR6GfmND07oUncUQoZjuK1zzB6Fn0OtJzCoPE8eclQcLBR1hE45yd0onhCQ2InwuRrjMlw+QcBI5gZHuTXfS2FfV9z2r4nSTBPJuCyV8HUSX4vFitc+roRV/jsDrDbm1wWZO1GMQW73VAKw87EjokVe7I5bUQUAX56uGNTuJN9dLFSqkgp9RjwMEb/oou8LJemKfY7ZhI1EU9IzumE8YSGnPkwWAJgxUP1m8KD/Ltn/6O0daiPbmS3Gsq3Y58Bi9stwzqG6JFGe5SDq2Hzq00uCfY3U9ldLIWsnVCeC8Pns+toMVklVTrrqBncnbx2qojcqJRaA2zAxVQ0jZc5sBz6jGs0wnHTISOeMKuzxRMaEtoHTvut0Wbg4PeAkZZa0N0CzVm74P2rqAruz/XVv2PKCC+P22wr0242MthWPgLZSY12B/qbu09MIWWl8Tz8LNYeyEUEzhilmzK4wp3so0eB+zlesOYHvONNoTRNUFFgZB6NWNBo18ZD+QT7mxnX2eIJJzPjdug92Mh+sdUR0d0shYJUeOdSsIbw/oh/UyRhndd6E4FF/zHmaX98M9Se2Jww2N/SfWIKKasNF2ZIDD+l5DEmNqw++03TGHcshYuBC4FyAKXUMY631dZ0FCmrQNlgZON4QqerT3CFXwCc84SRopr4FhEh/t0nJbUsB96+2JiVcM0nrDhqYWy/MHoHdeIvn5BoWPQS5OwxhjU1IMhq7h4xhcoiSN8Mw8+moqaObUeKOHV4lK+l6tS48y1S4xibqQBEJNidE4vIYhHJEZHdDbZFOLquJjuewx3bRUSeF5EUEdkpIqe05Yfp1uz/xmjk1e/Ej+Z4PKETu44aMvpCGHwqfP8Usf7VFFbUYrd38VZaNeXwziVQlg2/+B9V4fEkHi5i9rAu8OUz4hyYdgts/I8jbdMg2N9CTZ2dOlvTgeguw6EfjJup4fPZklZIjc3OHK0UmsUdpfB/IvIK0FtEbgFWAa+5cdwS4GRfxx+B1UqpeGC14z3AeRgjOOOBW4H/unH+nkNdjWEpjDinURaLM57Qad0UJyMCC/4CFQXMy16Cza4oqeriGUj7vzFiCRe/AgOnkeD48uk0jQlb4pwnjOaFn94O5UZbjiB/x6Cd2i5uLaSsAmsvGDCNdSl5+JtNTIvrIv8rPsKd7KNngY+AjzHqEx5RSr3gxnFrgYKTNi8C3nK8fovjWUyLgKXKYCOGAopFY3BkvdGHv4lU1C4TT2hI7EQ45VrGpr/PEMns+q0u0jeDXxCMPB8wWltYTNJ1vnz8Ao001Yp8+OGvAARbHe2zu3KwWSkjnjBsHpgtrEvJ45TBvQn016mozeGWE1optVIp9Xul1O+UUivbcb0+SqlMx+sswFlS2B9Ib7AuAxcZTiJyq4gkiEhCbm5uO0TpQuxfDmYrDDuj0a4NXSWecDJnPozdbOVBy7tdP66Qsdlw65mNL9L1B/OZOLA3Idbmpt12MmInGB1c93wKtrp6S6G8Kwebc5KMeR7D51NQXsOeYyXM6QouPR/js2+ShnGKVh73qlJqqlJqanR0D6hIVAoOfGMMZvE/MZyTW1pNSleKJzQkJIbcyXdztjkRDv7ga2naTk2F4ToaOB2AkqpadmYUMaeruI4aMvYiqMiDI+uPD9rpypZCyirjefh8Nji61c6J10qhJTpaKWQ73UKO5xzH9qPAwAbrBji2aXL3Q2Fak6mom1KNP/QuE084ibppv+KwPYYhO571tSht59g2Y7aFQylsSS3ArmBWV7wjHX624Qbb8xnB3cFSSF4JMWMhrB8/peQRarUwoSu5WX1ERyuFZcD1jtfXA5832H6dIwtpJlDcwM3UszngqGLuyvUJLojoFcontrmEFye51c65U5Kx2XgeMA0wXEdWi4nJg3r7Tqa24h8E8efA3i8I8jP6NHXZWoXqUqOuJ34+YMR5ZgyN1F1R3cCd4rU5jvTRAyJySERSReSQG8e9j1H9PFJEMkTkl8DTwNkikgzMd7wH+Bo4BKRgZDbd0cafp/uxf7nRwKzX8RBLTZ2d9Sl5fL8vl2lDumA8wUGQv5kDpiEIyq3hL52S9C0QMRSCDctg/UFjRnaX7asz9iIozyEqPwGg6w7aSV1rTJobPp/0ggoO51dw6vAu6NLzAe5Ewt4A7gO2YgzbcQul1FUudp3VxFoF3OnuuXsM5fnGnehpvye7pIrv9+Xw/f4cfkrOo7zGhr/ZxMMXjPG1lG1GRDgWOAJqgMwd9S6YLoNSxu9nuHE3WlBew97MEn5/bhduIhx/DlgC6X34G+DsrhtTSFkF/iEwcCbrEo1usLo+wT3cUQrFSqlvvC6JphFHNn/GIGXnnsS+fP7tagBiewWwaHJ/zhgZw+xhkfWpg12V2qBYSuvCCM3a6WtRWk9hqtFozeE6cgYzu0x9QlP4B0P82QSlfIWJs7pmTEEpQykMOR0s/qw7mE9MqJXhMSG+lqxL4M43yvci8nfgE6C++b1SKtFrUmnYmVHE0e8+wGrqTVbwSO6f0pczRkUzsk9otxo0HhFi5WD5UCZldkGlkL7FeHZYOOsP5hHSHYKZYy/CtHcZU2U/FTWjfS1N68lLhqIjMOde7HbF+pQ8ThsR3a3+b7yJO0phhuO54ZR4BZzZxFqNh1i+4zB3mHZinnAZH146x9fieI3wYH/2EseknK+N+dNmP1+L5D4Zmw0XRYzhwttwMJ/pQyK6fjAz/lyUJYALbJvIqr7A19K0ngapqPuzS8kvr9Guo1bgTkXzGU08tELwMlm71xAiVQSOXehrUbxKVIg/idWDjEZyuft9LU7rSN9kDII3mcksruRQXjmzu7LryIk1BBk+nwXmLVRWd8EWJCkrIWoEhA9mXUoeAHN0kNltXCoFEbnG8fybph4dJ2LPIyWnjEGl21AIDJ7ta3G8SkxoAIm1jhKVrhRXqC4zMqYcriNnPKFLNMFzh7EXE0Mh0UXbfS1J66ipgLR1Rs0F8FNKHkOjg4ntFehjwboOzVkKzvLZUBcPjZdYmZTNdNlHbfQ4COzta3G8SnSolVQVi90SCF0prnAsEZQdBhre1fUH8wkP8mNU327yrzHiXGrwY3Thdy2v7Uyk/QS2ahh+FjV1djanFuhW2a3EZUxBKfWK4/nxjhNHA/D9nnRuMqfgP/SXvhbF68SEWrFjojx8dNfKQEp3Fq1NRSnFhoP5zBoWicnUTYKZ1lC2+k1lUukaY8ZxZ5kx3RLb3jaqsgfPYXt6ERU1tu5jvXUQXeQ33XPILa3GfjQRKzUQ130DzE5iwqwA5IeONHoIuRgk3+lI32z4rQPDOZhbxtGiyq7Z2qIZEoLnEm7LP1613dk5sAL2LoO5vwG/ANal5GGSTj6mthOilUInY/XebKbLXuPNoO4dTwAjpgCQYY032oMXpflWIHdQCjK2wIDplFfXcff72wm1Wpg/unvN/d0bNoca/CDp85YX+5qacvjqt8ZciNn3ALAuJY/x/XvRK6gLZbR1ArRS6GSsTMrmdGsyKno0BHf/O5zwID/8zEKyeaixoSvEFfIPQmUB9gHTueeDbezPLuXFq0/pdsFMU0AYW8yTDaXQ2S24H56G4iPws3+BxZ+y6jq2pxfpVNQ24E7vo0gReUFEEkVkq4j8W0S6/7eVD6ioqWNDSjaT2I/0ANcRGK0uokOs7K3rByZL18hASt8EwKupkazam8NjPxvD6SO6Xxv3YH8LK5kJJUfhaIKvxXFN1i7Y8B845br6bL3NqfnU2ZVWCm3AHUvhA4wW15cClwG5wIfeFKqnsvZAHsNth7DaK7p9KmpDosMCyCzHMP27gqWQsZkaSyh/26q4cU4c186K87VEXiHIauZb2ylg9oc9n/lanKax2+CLeyEwHOYfz4n5KdnoVjtlcLjvZOuiuKMUYpVSTyilUh2PJzk+MU3jQQzX0QHjzeCeYSkARIdYySmpNqZ/dQFLofzgBjbWDOGMUX15aGHXbUjYEkH+ZnJqrKhhZxguJNXqmVjeJ2GxYcUs+CsEHZ8rsv5gHtPiIrput1of4o5SWCEiV4qIyfG4HPjW24L1NOpsdr7bl825IQchYhiE9vW1SB1GTJiV3LJqo0V4WTaUZvtaJJccTD9GYOEBDgeN4/mrJmPuLimoTRDkb8FmV9SOXAQlGXB0q69FOpGSTFj9Zxg6D8b/vH5zbmk1+7JKma2rmNtEcxXNpSJSAtwCvIfRDK8aw510a8eI13NIOFxIcUU1I6t394hU1IbEhFopKK+hNma8saGTWgt5ZdW8+M4HmERx3nkXdq0ZzG2gfvpa3Dlg8jPmN3cmlv8R6qph4XPQoNnd+oNGawtdtNY2XCoFpVSoUirM8WxSSvk5HialVFhHCtkTWJmUzTjzUfxqS3qU6wiOp6XmhYwwNmTu8KE0TVNVa+PWpQkMqUxCIUSN7P4xnyCH0iuTYBh2BiQt6zwupAPfQtJncPrvIXLYCbvWpeQRFmBhbL8u3q3WR/gkJVVE7hORPSKyW0TeF5EAERkiIptEJEVEPhQRf1/I5guUUqxMyuaKmMPGhh6nFIwCtuwaK4THdTpLQSnFHz7aSeKRIq6MzUJiRkNA9//CCfY3lEJlrQ3GXGSkfB7rBB3za8rhq9+dUJPgRCnFupR8Zg+L6tauPW/S4UpBRPoDdwNTlVLjADNwJfA34J9KqeFAIdD9ezw4OJBdxpGCCub6J0OvQdB7oK9F6lCcVc05JVVGXKGTZSD9d81Blu04xh/OjSemaGf9UJ3uTpDV4T6qroNR5xspw50hC+mHv55Qk9CQw/kVHC2q1F1R20FzMYUhXryuBQgUEQsQBGRizGf4yLH/LeAiL16/U7EyKQtQDChJ7HHxBDjuPsopdWQgFaZCVbGPpTJISCvgHysOcMGEWG4fa4Pq4q43NrSNBDkydypqbEbK57AzYft7UFnkO6Eyd8KGl06oSSitqmVnRhGfbz/KP1Ya2Xu6PqHtNBcp+wiYIiKrlVKN5iq3FaXUURF5FjgCVAIrMOY/FymlnLP/MoD+TR0vIrfiCHQPGjTIU2L5lJVJ2VwQW4qpML9H1Sc4iQzxR8ShFAZPNDZm+T7gXlRRw93vb2NAeCB/vWQ8kvS+sWPgjOYP7CY4R72WVzv+Lc98CF6dB989CQuf9YlM5V/9CZMljGcqr2DPKxtIzSsnt7R+ICQmMWYnDIkKbuYsmuZoTimYRORBYERT8xOUUs+15YIiEg4sAoYARcD/gAXuHq+UehV4FWDq1KmdJOrVdrJLqtiRUcx9448YTrMeFk8A8DObiAjyN/65YycYG7N2+lQpKKX4/Uc7yS2r5uPbZxMa4Gc0wQsMh8jhPpOrIwnyb2ApAMROhGm3wJbXYPLV0G9yxwpUVYI1Yx2v1i1k2YFKhkQFc8bIaIZEhTAkKphh0cEMigzCatG1Ce2hOaVwJYYLx4Jn5yfMB1KVUrkAIvIJMAfoLSIWh7UwADjqwWt2WlYmGTn5U9QeCOkLEUN9LJFviA61kltaZdRnBMf4PK7w1vo0ViZl8/AFY5gwoLexMX2zEU/oIbN+6y2FmrrjG8/8k5Ga+tVv4ZerOrSldlHSKnpjo9+UhWy9+OwOu25Po7l5CvuBv4nITqXUNx685hFgpogEYbiPzgISgO8x2mh8AFwPdIHWjO1nZVI2cRGBhGRvMe6Me8gXzsnEhAUY7iPweWXz7qPF/OXrfZw1Koab5sQZGysLIW//CUVS3R2npVDptBTAyLo69yn45BZIfAum3thh8hTsXI5FBRA/xWPebE0TuKPm14vIcyKS4Hj8Q0TanI+nlNqEEa9IBHY5ZHgVuB/4jYikAJHAG229RlehtKqW9Qfz+PmwOqT0WI+MJziJCXW0ugAjAyl3n1GY1MGUVddx13uJRAT78/efT0ScSjrDUc3bQ4LMYFQ0A5RX207cMf7nEDcXVj0G5XkdI4xS9D66hi0yjlEDdBDZm7ijFBYDpcDljkcJ8GZ7LqqUelQpNUopNU4pda1SqlopdUgpNV0pNVwp9XOlVMd/I3Qwaw7kUmtTnBeaamwYfKpvBfIhMaFW8sqqsduVYSnY6yAnqUNlUErx0Ke7OFJQwfNXTSYiuEG6Y/omEBP0n9KhMvkSs0mwWkxUNHQfgWHNnv8s1JTBqkc7RBaVn0JEbRZZUXN0/YGXcUcpDHN8iR9yPB4Heqbj28OsTMomItifuPJtEBQJ0SN9LZLPiAm1UmdXFFTUGJYCdHhc4X9bM/hs+zHunT+C6UMiTtyZsRlixoI1pENl8jXBVsuJMQUnMaNg1p2w7R04stHrchTsMDzYAaN1LMHbuKMUKkWk/hZWROZgxAI07aDWZuf7fTmcNSoG0+H1huuoh8YTwIgpAIYLKXwI+Id2aFwhJaeURz/fw6yhkdx5xknZRXab4T7qQa4jJ0H+ZipOdh85Oe0PEDbACDrbmlAcHqRq3wpS7X2YMKGDM556IO4ohduA/4hImoikAS8Cv/KqVD2AzakFlFTVcUGcHYoO98hU1IZEO1pd5JZVGxktfcd3mKVQVWvjrve2EeRv5l9XTmrsnsjdBzWlPVIpBPtbjqeknow1BM57GrJ3w+ZXvSdEXTVReVtIsJzCUF1/4HVabPOolNoBTBSRMMf7Eq9L1QN4f/MRQq0WZpn3Gxt6uFJw9j/KKakyNsROgMSlxl26ybt55/9YsZ99WaUsuXEafUKtRuvugkPHH073SA9pb9GQIKu5afeRk1EXwPCz4fu/wNiLISzW4zLY0zZgVVUU9597PPCv8Rpu9/7VysBzZBRW8M3uLH556hD8M74Cay/oM9bXYvmUE1pdgBFXqK0w5iFHj/DadZVSVGz7mI8jNzHlu7/CR4egtvz4AjEbvagm/qJH1pA0aymAI+j8DPxnJqz4E1y2uOl1tlrD4ipKhxHntkrRF+z8hjBlJnL8/FZKr2kL3bshfCdlybo0AG6YHQdvr4fBs7x+N9zZCfQ3E2q1HG9Z0LCy2YtKoXj1czxV9yxlllgImwBxpxpf/hFDIWII9B4EZj+vXb+zE+hvJq+shUTAiKEw9zdGo7pTroNBs4zMscwdcGy78Zy9B2yO81z4grHOTUyHVpNgH8nMUd2jrU1nRyuFDqa0qpYPtqSzcHws/cwlkJ8Mp1zra7E6BdFhVnJKHe6j6FHGbODMHTD+Ms9fTClY/Ti9f/onX9pmMvzadxnVX+e/n0ywv7l5S8HJnHthxwfwwdVGfYm91thu7WUo+Om3GG0xfnwONv4XJl/rXmJFSSYRZcnsDryO2b0C2/WzaNzDLaUgIrOBuIbrlVJLvSRTt+bDLemUVddxy9yhcOQ7Y2MPrk9oyAkFbGY/iBntnQwkuw2++g1sXcLG8EU8XHgVW2N1q+WmCLJaGtcpNIVfAFz0X1j3b+P3FjvReITHnfjlX1cFn98JqWth6OktnrYuZTUWoDrujDb/DJrW0aJSEJG3gWHAdsB5y6AArRRaSZ3Nzpvr0pgxJILxA3rBjnXgF2z882iICQ1ge3rR8Q19J8C+r4y7ek8FGOuq4ZNbjaldc3/LA4lzmTIkFJMuiGoSty0FMNygg2c1v2bcZbDyUcNacEMplOz6hjrVm2HjZrong6bduJOSOhWYo5S6Qyn1a8fjbm8L1h35ZncWR4squXmuI2B5eD0MmgFm7cUDw1LILa1GOUc+xk6EygIo8VBvxOoyeO8KQyGc8xQ50/5Aan4FM04uVNPUE+QINNvtHmpI7BcA034JB5YbSQTNYbcRmP4jP9rHM2uYdu11FO4ohd1AX28L0t1RSvH6j4cYEhXMWaNioKIAcvb0+FTUhkSHWqmstVHm7N/vycrmigJ4+yJIXQOL/gOz72JTagFA4+plTT3BjulrlbVuWgvuMPWXxhS3Ta80v+7YdgLrijkYNpPw4B4zndfnuKMUooAkEflWRJY5H94WrLuxJa2QHRnF3HTqEMNVcXidsUMrhXrqx3I6M5D6jAWk/XGFkkx483wjaH35Uph8DWAUEAb7mxnbL6x95+/GBPo30T67vYT2gXGXwvZ3m52wV3tgJXYl+I3QXVE7Enf8Fo95W4iewOs/HqJ3kB+XnTLA8JFveMmYG9D/FF+L1mmor1UoqWZYdIhRMRs5vH2Wgt0GSy+EkmNw9Ucn+LE3pxYwJS4Ci7nDR5V3GYKdg3aqbZ6dqjLzNtj5gdE7adadTS6pSPqWNDWESaOGefDCmpZwp6J5TUcI0p1JzStn5d5s7pw3nEB/MySvhCPrjU6TFquvxes01Fc1O9NSwUhnTN/c9pMe/A7yDsClb5ygEArLa9ifXcqFk/q1/dw9gCBvWApgpKcOmg2bXoYZtzWu06ksIjRvBz+qC7kxTrv3OpIWb5FEZKaIbBGRMhGpERGbiOjq5lbw5rpU/Ewmrps9GOx2WPW4kap3yvW+Fq1T4bQUGs7cpe8EKE6H0qy2nXTrEgiKgtEXnrB5c5qOJ7hDfUzB3Qyk1jDzdig6Avu/brwvdQ0mbGRHz66fAKfpGNyxm18ErgKSgUDgZuA/7bmoiPQWkY9EZJ+I7BWRWSISISIrRSTZ8Rzenmt0FooqavhfQgaLJvUzvvR2fwzZu+DMh8Gig2cNCQu04G8xHY8pAIxaaDxvfav1JyzNNrJcJl3V6LPenFqA1WJiwoA2z4vqERy3FLygFEYthF6DjPTUk6jZt4JSFUjUSF3D09G45UxVSqUAZqWUTSn1JrCgndf9N7BcKTUKmAjsBf4IrFZKxQOrHe+7PO9uOkJlrY1fzh0CdTXw/ZPQZzyMvcTXonU6RMRRwNbAfRQVD8PnQ8IbxufXGna8bwzrmdy4pcLm1AImD+qth7y3gNNSqKj2Qmtskxlm3GokXWTuOL5dKWzJq/nJPo5ZI3TiY0fjjlKoEBF/YLuIPCMi97l5XJM4RnmehmPcplKqRilVBCwCnLeDbwEXtfUanYXqOhtL1qcxNz6KUX3DjJm2hWkw/9EOHXjelYgOtRrtsxsy43YoyzbqC9xFKaPL6qBZjXonlVbVsudYMdOH6Crmlgjy86KlAEa7C79g2Pjy8W15BwiszGSDTGLSwN7eua7GJe58M13rWHcXUA4MBC5txzWHALnAmyKyTUReF5FgoI9SKtOxJgvo045rdAq+2JFJbmm1UaxWUw5rnjFSUIfrbo+uOKHVhZNhZ0JkvOFmUG4WUR1eBwUHm4zbbD1ciF2hi9bcIMhpKXg60OwksDdMvhp2f2S4+wBSVgFQOuB0/C365qmjafETV0odBgSIVUo9rpT6jcOd1FYswCnAf5VSkzEUzQmuImWUtDb53y8it4pIgogk5ObmtkMM7+IsVhvRJ4TT4qNg40tQngPzH+vRE9ZaIiY04MSYAhhW1YxfwbFEyNji3okSl4I1DMYsarRrU2oBFpMweVDv9gvczQl2xhRcTV/zBDNuA1sNJBhtt6v3rSTF3o9RI8d475oal7iTffQzjL5Hyx3vJ7WzeC0DyFBKbXK8/whDSWSLSKzjGrFATlMHK6VeVUpNVUpNjY6ObocY3mVdSj77skq5+dShSGUhrHseRi7skdO7WkNMqJXiylqqTq6gnXiV0XGziaBkIyoLIelzGP9z8A9qtHtzagHjB/SqD6JqXBPgZ0IEKr1lKQBEDoP4c424UWURlvT1rLVPYM5w3drCF7hjmz0GTAeKAJRS2zFcQG1CKZUFpIuIc0r9WUASsAxw2vrXA5+39RqdgVfWHiQqxMqiyf3gp+eguhTOetjXYnV6nFXNuSdbC9YQo8V40udQ3EIvpJ3/M7pxTmnsOqqssbEzo4gZOp7gFiJCsL/FezEFJzNvh/Jc+Pp3mO3VJFhOYXSsrjT3Be4ohVql1Mm16O3tjvVr4F0R2QlMAv4CPA2cLSLJwHzH+y7JroxifkzO46ZT47CWZ8GmV4073ZjRvhat09NoAltDpt8CKOOO0hVKGQF9Z+vmk9iWXkitTel4QisI8jd7L6bgZOg8iB4Nu/5HNX6Yh5zaeFa2pkNwRynsEZFfAGYRiReRF4D17bmoUmq7wwU0QSl1kVKqUCmVr5Q6SykVr5Sar5QqaM81fMlLP6QQGmDhmpmDjWlUKDjjAV+L1SWIDnVaClWNd4bHwcjzIeFNqK1s+gTHthmD5F1M9tqcWoAITInrFmUwHUKQv9m7MQUw4mwzbwdgk20U00f09+71NC5xRyn8GhgLVAPvAyXAvV6UqUuTklPG8j1ZXD8rjrDSVKPp17SbjbGOmhZx6T5yMuM2o532rv81vT/xLbAEGvGEJth0qIAxsWGEBfTcEZutxWif7WVLAWDC5RSGjuQT21xm63iCz3An+6hCKfUnpdQ0x939n5RSTdzGaQBeXnMQq8XEjXPi4LsnwC8I5v7W12J1GSKDrZjEhfsIjBnKfcYZee0np6dWl8Guj2DsxRDQuFK5ps5O4pFC3dqilQRbO8BSAPAL5KHYl9kYMp+hUcHev56mSVymX7SUYaSUurC5/T2RjMIKPtt2lGtmDiayaDfsXQbzHoBgfdfjLmaTEBnSRK2CExHDWlh2F6T9CENOO74v6TOoKXPpOtp1tIjqOrsOMreSIH8LRZW1Xr9OdZ2NDQfzmTciGtFp2z6juZy8WUA6hstoE0atgqYZXlt7CIBb5w6Bzy83GrG5aAuscU1MqPXETqknM/7nsOpRw1poqBS2vgVRI2BQ06MbnUN1pul4QqsItpo5VuQihuNBXllziILyGi45ZYDXr6VxTXPuo77Ag8A4jF5FZwN5Sqk1up12Y/LKqvlgSzoXT+5Pv+wfjLvYeX8Eqyeb0PcMDKXgwlIAY6TjlBuM7pqFaca2nL2QsdmwElzcZW5OLSA+JoTIEN2uvDU4R3J6k8P55bz4fQoLJ8Ryary2rH2JS6XgaH63XCl1PTATSAF+EJG7Oky6LsTin1Kpsdm5be4gWPmI0ZZhyg2+FqtL0mRV88lMu9loqLb5NeN94ttg8oMJVza5vM5mJyFNxxPaQpC/2fPzFBqglOLRZXvwMwkPL9RVzL6m2UCziFhF5BLgHeBO4Hng044QrCtRUlXL2xsOc/64WIYd+Qjyk+GcJ8CsM1zaQkyYlfyyamzNDYsP62e0sEh825i/vON9GHU+hDRd5b43s5Sy6jpmDNXxhNYS5G8xJq95iW/3ZPHD/lx+c85I+vYK8Np1NO7hUimIyFJgA0YLiscd2UdPKKVaKCfteby94TCl1XXcOTvaqEuImwsj2ttdvOcSE2rFriC/vAVrYcbtUF0M/3edkabazNCiTan5AEzXU7xaTbC/mRqbnVqb3ePnLq+u4/EvkhgdG8b1swZ7/Pya1tOcpXANEA/cA6wXkRLHo1RPXjtOZY2NxT+lcvqIaMakvA4V+XDOk7rpXTtwFrC5zEByMnAa9J9ixG96DYKhZ7hcujm1gMGRQfpOtA0EOSafeSOu8O/VyWQWV/HkReP0rOxOQnMxBZNSKtTxCGvwCFVK6aYkDj7ccoT88hrumxZgNGubcCX0m+Rrsbo00U2N5XTFDKMKlsnXuJxRYbcrtqQVaCuhjQT7e6d99v6sUt74KZUrpw1kymCdEdZZ0G0i20FNnZ1X1x5iWlw4kw48b1gHuuldu4lxWgrNpaU6GXux4UIaf7nLJck5ZRRW1OogcxtxWgqeLGCz2xUPfbaLsAAL9y8Y5bHzatqPttfawefbj3KsuIr7x5cbbRdm3Qm9dI51e3HbfQRgthiZSAGujdfNjnjCTB1kbhNBfp63FD5OzGBLWiEPnDea8GA9q7wzoZVCG7HZFf9dc5DRfUOZsv8fEBwNp97na7G6BQF+ZnoF+rWcluomm1ILiO0VwIDwQI+cr6fhnL7mKUuhsLyGv36zjymDw7lsir6J6mxopdBGVuzJ4lBuOU+MTEWObIAzHtSFah6kxapmN0k6VsKPyXlMi4vQrRPaiHP6mqcshWe+3U9xZS1PXjQOk26P3enQSqEN2OyK579LIT7SnynJ/4LoUTC56X47mrYRE2Z1L9DcDF/tzOTS/64n0M/MXWcO95BkPY/g+jnN7bcUEo8U8v7mI9w4O04P0emk6EBzG1iyPo29mSV8OX03svMQ/OJ/hm9b4zFiQgPYkta2kRp2u+Kfqw7wwncpnDKoNy9fO6V+eI+m9QR5yFKos9l56NPd9A0L4N6zR3hCNI0X8Nk3mYiYgQTgqFLqAhEZAnwARAJbgWuVUjW+ks8VGYUV/GPFfhYOD2Bs8n9hyOkQf7avxep2RDv6HymlWuX2Ka2q5b4Pt7Nqbw5XTB3Iny8ai9Vi9qKk3R+n+6i9MYWvdmWSlFnCi7+YTIhV30R1VnzpProH2Nvg/d+AfyqlhgOFwC99IlUzKKV45PM9KAVPx6xAKot0oZqXiAm1UlNnp6TS/bvTtLxyLnlpPd/vz+XxC8fy9KXjtULwAIEeqlN4a30acZFBnD8u1hNiabyET5SCiAwAFgKvO94LcCbwkWPJW8BFvpCtOb7elcV3+3J4Zno5odteg8lXQ+wEX4vVLYluTa0CsPZALhe++BN5ZdW8/cvpXD87TgeWPYS/xYSfWShvR0xhV0YxiUeKuG5WnA4ud3J8ZcP9C/gD4EzXiQSKlFLOW5EMoMkhrSJyK3ArwKBBHTfisriilkeX7WF2rHBB8kPGeM1z/9ph1+9pOGMAOaXVxPdpPqvr9R8P8Zev9zKiTyivXTeVgRFBHSFit6C2tpaMjAyqqppXvi9f0Jcg/xr27t3b7DpXlJbX8MaiWPr2qmzzOTStJyAggAEDBuDn535zzg5XCiJyAZCjlNoqIvNae7xS6lXgVYCpU6c200bTszy9fB+FFdW83O8NJCMXbl7ZbMGUpn04ZzW3ZCmsS8njya/2smBsX/5x+USCta+6VWRkZBAaGkpcXAuWVWYJIVZLmxRurc3OvqxSBgf501/XinQYSiny8/PJyMhgyJAhbh/ni/+gOcCFInI+EACEYQzx6S0iFoe1MADoNN1YN6cW8P7mI7w2YhNhR1bDgr9Bv8m+FqtbE+NmVfMraw8RFWLl31dN0vGDNlBVVdWyQgBMIthPnontJoXlNSiliAzRlcsdiYgQGRlJbm5uq47r8JiCUuoBpdQApVQccCXwnVLqauB74DLHsuuBzztatqaorrPxwCc7ObtXOvMzXoJRF8CMX/larG5PiNVCoJ+52VqFvZklrD2Qy41z4rRCaAfuxF7MJmhuvIUr7EqRX15DiNVCgJ/+HXU0bYmrdabitfuB34hICkaM4Q0fywPAf384SG5uDs9bXkBCY2HRizrbqAMQkfq0VFe89uMhgvzNXD2j42JLPRURwd4GrVBSWUutzU6UHoHaZfCpUlBK/aCUusDx+pBSarpSarhS6udKKc80vmkHKTmlvPR9Cm9Fv0NgZRZcthgCdYvfjqK5VheZxZUs236My6cOpHeQdkt4G3Mb3Uf5ZTX4m02EBrjvqY6LiyMvL69V13n++ecZPXo0V199NdXV1cyfP59Jkybx4YcfMm/ePBISElo8x2WXXcahQ4cAmD9/PoWFha2SwVts376dr7/++oRtn332GX/+858BePHFF1m8eLHHrteZLIVOhd2uePCT3Vznt5rJpWvgrEeMoS6aDiMmzLWlsGRdGnal+OWp7gfQNG3HiCm07pjKGhvlNXVEhvh7PT34pZdeYuXKlbz77rts27YNML5Mr7jiCreO37NnDzabjaFDhwJw7bXX8tJLL7V4nM3mvTGlTppSCs888wx33HEHADfddBMvvPCCx66nUzVc8GFCOmWHE3kgcCkMPRtm/drXIvU4YkID+PFA4zvG0qpa3tt0hPPHx+r0Uw/y+Bd7SDrW9FDF6jo7NrsiyN/9uEB1nZ1BEUE8d/lEl2vKy8u5/PLLycjIwGaz8fDDxjySF154gS+++ILa2lr+97//MWrUKB577DFCQkL43e9+B8C4ceP48ssvefrppzl06BDnnXce11xzDa+99hq5ublMmjSJjz/++ITrrVixgkcffZTq6mqGDRvGm2++SUhICO+++y6LFi2qX3fhhRcyd+5c/vSnPzWSOS4ujiuuuIKVK1fyhz/8gYiIiCbPGRcXx+WXX84333xDYGAg7733HsOHDyc3N5fbbruNI0eOAPCvf/2LOXPmsHnzZu655x6qqqoIDAzkzTffZMiQITzyyCNUVlby008/8cADDzB58mSsVitRUVEABAUFERcXx+bNm5k+fbrbvx9XaEuhCYoqavj314m8EfQfTMGRcPHLLqd6abxHdKiV0uo6Kk8qmvpgczql1XXcetpQH0nW8zDu8903FRRQZ7djtZiaHbO5fPly+vXrx44dO9i9ezcLFhizzaOiokhMTOT222/n2WefbfZaL7/8Mv369eP777/n/vvv5/XXX2fu3Lls376dYcOG1a/Ly8vjySefZNWqVSQmJjJ16lSee+45ANatW8eUKVPq14aHh1NdXU1+fn6T14yMjCQxMZH58+e7PCdAr1692LVrF3fddRf33nsvAPfccw/33XcfW7Zs4eOPP+bmm28GYNSoUfz4449s27aNP//5zzz44IP4+/vz5z//mSuuuKLe8lm3bh2nnHLKCfJMnTqVH3/8sdnPyV20pdAEH24+wv22V+krmcilX0BwlK9F6pE0nMA2ODIYMHLeF69LZebQCCYM6O1D6bofj/5srMt9WcVV5JZWMa5/L7dcQTmlVWQVV7VYeDh+/Hh++9vfcv/993PBBRcwd+5cAC655BIApkyZwieffNKKn8I1GzduJCkpiTlz5gBQU1PDrFmzAMjMzCQ6OvqE9TExMRw7dozIyMbDmZxuqebOCXDVVVfVP993nzFvZdWqVSQlJdWvKSkpoaysjOLiYq6//nqSk5MREWpra5v8OVzJum/fPvc/jGbQSuEk6mx2sta9za/M6+D0ByHuVF+L1GOJCTs+q9mpFL7ceYzM4ir+cvF4X4rW4zCZjLt/pVpOvlNKUVBWQ7Ajrbg5RowYQWJiIl9//TUPPfQQZ511FgBWq3FDYDabqaszGh1YLBbsdnv9sS1VYTcl19lnn83777/faF9gYGCj8zndOE0RHBzc4jnhxJRQ52u73c7GjRsJCDixc+9dd93FGWecwaeffkpaWhrz5s1r8pyBgYEUFxe7LWtr0T6Rk/hp63buq3mVwsjJcNrvfC1Oj+a4pWAEm5VSvLLmEPExIZw+Irq5QzUexuT8QnMjA6mkqo4am50oN4rVjh07RlBQENdccw2///3vSUxMdLk2Li6ufn9iYiKpqaluSm8wc+ZM1q1bR0pKCmDEMw4cOADA6NGj67eD8beWlZVFXFxcm88J8OGHH9Y/Oy2Ic84554TA8Pbt2wEoLi6mf3+ju8+SJUvq94eGhlJaWlr//mRZAQ4cOMC4ceNa/AzcQSuFhtjtRK66D4vYCfvFm2DSxTa+5PisZuMO7qeUPPZllXLLaUN1U7UOpjVKIb+sGj+zibCAlvvt7Nq1i+nTpzNp0iQef/xxHnroIZdrL730UgoKChg7diwvvvgiI0a0biZDdHQ0S5Ys4aqrrmLChAnMmjWr3uWycOFCfvjhh/q1W7duZebMmVgshjPl/PPP59ixY606J0BhYSETJkzg3//+N//85z8BI302ISGBCRMmMGbMGF5++WUA/vCHP9QHkp3WEcAZZ5xBUlJSfYrtaaedxrZt21ANfhfr1q3j7LM91MJfKdVlH1OmTFGe5NjyZ5V6NEytef9Zj55X0zZsNrsa9sBX6m/f7FVKKXXN6xvVtCdXqqraOh9L1n1ISkpya11RebXakV6oKmua/+wra+rUjvRClV1S6QnxOoyKigo1Y8YMVVdn/Hx33323WrVqVbvOOXjwYJWbm+sJ8Rpx9913q5UrVyqllEpMTFTXXHONy7VN/Y6BBOXie1VbCk5y9hK18Wm+U1OY+DOdftoZMJmEqBCjVsE5a/kG3dLCJzgtM1sLxQp5ZdWICBFdrKAwMDCQxx9/nKNHjZZr48aNq49vdEYefPBBKioqACOr6oknnvDYuXWgGaCuhrqPbqbEHsCmcY9yZnDX+oPuzjgL2OpbWkwf7GuReiTuuI/qbHaKKmoJD/RrNg21s3LuuefWv77lllvafb60tLR2n8MVffr04cILLwTwnNvIgVYKAGuexpKzmz/W/ob7Tz+l5fWaDiMm1MrOjGIKymu4blYcvYLc7wuv8RzOEE5zhkJuWTV23Q21y9P11LmnObIR9dM/WWY6i6phCxge03xetaZjiQ4NMGY1AzedGudrcXosLVkKJVW15JZWExHkT6C/vtfsyvRspVBdCp/+iorAfjxQ8QtunBPna4k0J+FMS104PpYB4bqlha9wxhSa6pRaU2cnvaCCAD8z/XrrITpdnZ6tFL59EAoP8xfrPURHRjJvRIyvJdKcxKCIIETQLS18zHFL4cTtdqU4UlABCgZHBOlU4W5Az1UK+7+BxKXkTLyddzP7c/1sPVC8M3LhpH6svO90xvXv5WtRejTHYwonaoWs4ioqauoYEB6ItQsM0fnss89OaDEBcO+997J27VoArrzySpKTk90+32OPPdZib6aT2bdvH5MmTWLy5MkcPHjwhLbfS5Ys4a677nLr59Ctsz1JWS4s+zX0Gc/fqy8hxGrhsikDfC2Vpgn8zCaGx4T4Wowej4g0GslZVFFDXlk1USFWenkgBVUpdUIbC29wslLIz89n48aNnHbaaQDcfvvtPPPMM16X4bLLLmPbtm0MGzbshLbf7tKtWmeLyEBgKdAHo53Kq0qpf4tIBPAhEAekAZcrpbwz5SLtR6ipIH/BC3z2eiZXzxhMqBvVlxpNt+abP0LWLpe7h9TUYTEJWMzYlcKv1ka8CAF+Jpx9VBvRdzyc97TLc6alpXHuuecyY8YMtm7dyuWXX86XX35JdXU1F198MY8//jhpaWksWLCAKVOmkJiYyNixY1m6dClBQUFs3bqV3/zmN5SVlREVFcWSJUuIjY3ltdde49VXX6Wmpobhw4fz9ttvs337dpYtW8aaNWt48skn+fjjj1m9enV9Z1aAuXPncsMNN1BXV1dfzezk+eef5+WXX8ZisTBmzBg++OADAJKSkpg3bx5Hjhzh3nvv5e677yYtLY0LLriA3bt3A/Dss89SVlbG9OnT+de//oXZbGb16tWMHDmyvu33TTfdRHj48SFerlpsHzhwoNu1zq4DfquUGgPMBO4UkTHAH4HVSql4YLXjvXcYdwncu4u3DwZTa1NcPzvOa5fSaLoLgqMpHoqqOqOdudViQlwpBDdJTk7mjjvu4J///CdHjx5l8+bNbN++na1bt9a7dfbv388dd9zB3r17CQsL46WXXqK2tpZf//rXfPTRR2zdupWbbrqpfv7BJZdcwpYtW9ixYwejR4/mjTfeYPbs2Vx44YX8/e9/r2+rfXLLbJPJxPDhw9mxY0cjOZ9++mm2bdvGzp0761tTgOEO+vbbb9m8eTOPP/64y+6mYLTLuO2227jvvvv4/vvvT2j77eyi6sRVi+1u1zpbKZUJZDpel4rIXqA/sAiY51j2FvADxtxmr1BjDeedjds4Y2Q0Q6KCvXUZjabr0MwdPUBGdilWiwmTCIUVNQyJCsbkAQt78ODBzJw5k9/97nesWLGCyZMnA1BWVkZycjKDBg1i4MCB9e2pr7nmGp5//nkWLFjA7t2764u3bDYbsbGxAOzevZuHHnqIoqIiysrKTihMa0hzLbMbKguACRMmcPXVV3PRRRdx0UUX1W9fuHAhVqsVq9VKTEwM2dnZ7f5MwHWL7W7dOltE4oDJwCagj0NhAGRhuJeaOuZW4FaAQYPaPrD9q13HyCur5sY5epyjRuMOJhFKq+qwK0VMWIDHXK4N21A/8MAD/OpXvzphf1paWqMZDiKCUoqxY8eyYcOGRue84YYb+Oyzz5g4cSJLliw5odldQ1rTMvurr75i7dq1fPHFFzz11FPs2mW42pxtvuF4q+/2tvkG1y22u23rbBEJAT4G7lVKnTAD0NGwqckqGaXUq0qpqUqpqSdrS3dRSvHmujSGRQczN14P0NFo3MEkRvZRiNVCn1Brywe0knPPPZfFixdTVlYGwNGjR8nJyQHgyJEj9V/+7733HqeeeiojR44kNze3fnttbS179uwBoLS0lNjYWGpra08I4La1DbXdbic9PZ0zzjiDv/3tbxQXF9fL2RR9+vQhJyeH/Px8qqur+fLLL1v9ebhqsd0tW2eLiB+GQnhXKeUcq5QtIrGO/bFAjreuvy29iJ0ZxdwwO87rA8U1mu6Cn9mEn9nEwIggr/zfnHPOOfziF79g1qxZjB8/nssuu6z+C3zkyJH85z//YfTo0RQWFnL77bfj7+/PRx99xP3338/EiROZNGkS69evB+CJJ55gxowZzJkzh1GjRtVf48orr+Tvf/97fTroyS2zs7OzCQwMpG/fvgDcfPPNJCQkYLPZuOaaaxg/fjyTJ0/m7rvvpnfv3q4/Kz8/HnnkEaZPn87ZZ599ggzu4qrFdrdrnY0Rr1oK/Ouk7X8H/uh4/UfgmZbO1dbW2Qlp+eraNzapsqraNh2v0XQX3G2drZTRyrzWZvOiNE2Tmpqqxo4d67Xzz5kzRxUWFiqllHruuefU66+/7rVreYru1jp7DnAtcKaIbHc8zgeeBs4WkWRgvuO9V5gyOIKlN00n2Kp7tGg07mIyCRZT9ytt+sc//lGf9tm7d2+uv/56H0vUMt5snS3KjUlKnZWpU6eqhIQEX4uh0XRZ9u7dy+jRo30thsaLNPU7FpGtSqmpTa3vfmpfo9G0iq58Y6hpnrb8brVS0Gh6MAEBAeTn52vF0A1RSpGfn98opbUltFNdo+nBDBgwgIyMDHJzc30tisYLBAQEMGBA6/q6aaWg0fRg/Pz8GDJEF3BqjqPdRxqNRqOpRysFjUaj0dSjlYJGo9Fo6unSdQoikgscbuPhUUCeB8XxFl1BTi2jZ9AyegYtY8sMVko12TyuSyuF9iAiCa6KNzoTXUFOLaNn0DJ6Bi1j+9DuI41Go9HUo5WCRqPRaOrpyUrhVV8L4CZdQU4to2fQMnoGLWM76LExBY1Go9E0pidbChqNRqM5Ca0UNBqNRlNPj1QKIrJARPaLSIqI/NHX8jSFiKSJyC7HEKJOMTRCRBaLSI6I7G6wLUJEVopIsuM53JcyOmRqSs7HROToSYOdfCXfQBH5XkSSRGSPiNzj2N5pPstmZOw0n6NDngAR2SwiOxxyPu7YPkRENjn+xz8UEf9OKOMSEUlt8FlO8pWMDelxMQURMQMHgLOBDGALcJVSKsmngp2EiKQBU5VSnaYIR0ROA8qApUqpcY5tzwAFSqmnHQo2XCl1fyeU8zGgTCn1rC9lc8gSC8QqpRJFJBTYClwE3EAn+SybkfFyOsnnCCDGsOhgpVSZY/b7T8A9wG+AT5RSH4jIy8AOpdR/O5mMtwFfKqU+8oVcruiJlsJ0IEUpdUgpVQN8ACzysUxdAqXUWqDgpM2LgLccr9/C+OLwKS7k7DQopTKVUomO16XAXqA/neizbEbGToVj5HCZ462f46GAMwHnl62vP0tXMnZKeqJS6A+kN3ifQSf8Y8f4o1khIltF5FZfC9MMfZRSmY7XWUAfXwrTAneJyE6He8nnbi4AEYkDJgOb6KSf5UkyQif7HEXELCLbgRxgJXAQKFJK1TmW+Px//GQZlVLOz/Ipx2f5TxGx+k7C4/REpdBVOFUpdQpwHnCnwyXSqVGGL7Kz3gH9FxgGTAIygX/4VBpAREKAj4F7lVIlDfd1ls+yCRk73eeolLIppSYBAzA8AaN8K1FjTpZRRMYBD2DIOg2IAHzqdnXSE5XCUWBgg/cDHNs6FUqpo47nHOBTjD/2zki2w//s9EPn+FieJlFKZTv+Me3Aa/j483T4lj8G3lVKfeLY3Kk+y6Zk7GyfY0OUUkXA98AsoLeIOIeIdZr/8QYyLnC46JRSqhp4k07yWfZEpbAFiHdkJ/gDVwLLfCzTCYhIsCO4h4gEA+cAu5s/ymcsA653vL4e+NyHsrjE+WXr4GJ8+Hk6Ao9vAHuVUs812NVpPktXMnamzxFARKJFpLfjdSBGAslejC/eyxzLfP1ZNiXjvgY3AIIR8+gU/+M9LvsIwJFG9y/ADCxWSj3lW4lORESGYlgHYIxMfa8zyCgi7wPzMNr+ZgOPAp8B/wcMwmhjfrlSyqdBXhdyzsNweSggDfhVA/99R8t3KvAjsAuwOzY/iOGz7xSfZTMyXkUn+RwBRGQCRiDZjHGT+39KqT87/oc+wHDLbAOucdyRdyYZvwOiAQG2A7c1CEj7jB6pFDQajUbTND3RfaTRaDQaF2iloNFoNJp6tFLQaDQaTT1aKWg0Go2mHq0UNBqNRlOPVgqaboWI2BwdJ3eISKKIzG5hfW8RucON8/4gIj4btC5G19woX11f03PQSkHT3ahUSk1SSk3EaCPw1xbW9wZaVApdmQaVvRpNi2iloOnOhAGFYPTwEZHVDuthl4g4O+M+DQxzWBd/d6y937Fmh4g83eB8P3f0xT8gInNPvpiIzHNYFB+JyD4ReddRrXrCnb6ITBWRHxyvHxORt0TkRxE5LCKXiMgzjusvd7SacPIHx/bNIjLccXy0iHwsIlscjzkNzvu2iKwD3vbgZ6rp5ug7CE13I9DRjTIAiMVooQxQBVyslCpxfDlvFJFlwB+BcY5mZYjIeRgtrGcopSpEJKLBuS1KqemOivhHgflNXH8yMBY4BqwD5mD0z2+OYcAZwBhgA3CpUuoPIvIpsBCjahygWCk1XkSuw6jIvwD4N/BPpdRPIjII+BYY7Vg/BqOxYmUL19do6tFKQdPdqGzwBT8LWOroSCnAXxzdZu0YrZSbak09H3hTKVUBcFKbCWfjuq1AnIvrb1ZKZTiuv92xriWl8I1SqlZEdmG0Qlju2L7rpOu83+D5nw3kHeMwSADCHJ1NAZZphaBpLVopaLotSqkNDqsgGjjf8TzF8QWchmFNtAZn7xwbrv93GvbXabiujuPu2pOvW+2Q1y4itep47xn7SddRTbw2ATOVUlUNT+hQEuUufxKNxgU6pqDptojIKIw773ygF5DjUAhnAIMdy0qB0AaHrQRuFJEgxzkauo/aQxowxfH60jae44oGzxscr1cAv3YukE4y51fTddGWgqa74YwpgOEyul4pZRORd4EvHC6aBGAfgFIqX0TWichuDDfO7x1frAkiUgN8jdEdtL08DrwhIk8AP7TxHOEishPDsrjKse1u4D+O7RZgLcbsX42mTeguqRqNRqOpR7uPNBqNRlOPVgoajUajqUcrBY1Go9HUo5WCRqPRaOrRSkGj0Wg09WiloNFoNJp6tFLQaDQaTT3/DzdpvTDI99HmAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# You can find that a repeat before a shuffle mixes the epoch boundaries in this graph.\n",
    "shuffle_repeat_ds = raw_dataset.shuffle(32).repeat(3).batch(16)\n",
    "repeat_shuffle_ds = raw_dataset.repeat(3).shuffle(32).batch(16)\n",
    "\n",
    "shuffle_repeat = [batch[1].numpy().mean() for batch in shuffle_repeat_ds]\n",
    "repeat_shuffle = [batch[1].numpy().mean() for batch in repeat_shuffle_ds]\n",
    "\n",
    "plt.plot(shuffle_repeat, label=\"shuffle().repeat()\")\n",
    "plt.plot(repeat_shuffle, label=\"repeat().shuffle()\")\n",
    "plt.xlabel('Batch number')\n",
    "plt.ylabel(\"Mean of b in each batch\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb770237",
   "metadata": {},
   "source": [
    "Now, let's start designing our cnn model!\n",
    "\n",
    "## CNN Model for CIFAR 10\n",
    "### Loading Data Manually\n",
    "To know how it works under the hood, let's load CIFAR-10 by our own (not using tf.keras). According the descripion, the dataset file is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9b87d6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# the url to download CIFAR-10 dataset (binary version)\n",
    "# see format and details here: http://www.cs.toronto.edu/~kriz/cifar.html\n",
    "DATA_URL = 'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz'\n",
    "# the image size we want to keep\n",
    "IMAGE_SIZE_CROPPED = 24\n",
    "IMAGE_HEIGHT = 32\n",
    "IMAGE_WIDTH = 32\n",
    "IMAGE_DEPTH = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b4be4761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 576s 3us/step\n"
     ]
    }
   ],
   "source": [
    "# donwnload data\n",
    "cifar10 = utils.get_file('cifar-10-python.tar.gz',\n",
    "                                  cache_subdir=os.path.abspath('.'),\n",
    "                                  origin = DATA_URL,\n",
    "                                  extract = True)\n",
    "DEST_DIRECTORY = os.path.join(os.path.dirname(cifar10),'cifar-10-batches-py')\n",
    "filenames_train = [os.path.join(DEST_DIRECTORY,'data_batch_%d' % i) for i in range(1,6)]\n",
    "filenames_test = [os.path.join(DEST_DIRECTORY,'test_batch')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "73828fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        raw_data = pickle.load(fo, encoding='bytes')\n",
    "    return raw_data[b'data'],raw_data[b'labels']\n",
    "\n",
    "# parse training data\n",
    "def map_fun(image,label):\n",
    "    image = tf.reshape(image,[IMAGE_DEPTH,IMAGE_HEIGHT,IMAGE_WIDTH])\n",
    "    image = tf.divide(tf.cast(tf.transpose(image,[1,2,0]),tf.float32),255.0)\n",
    "    label = tf.one_hot(label,10)\n",
    "    distorted_image = tf.image.random_crop(image, [IMAGE_SIZE_CROPPED,IMAGE_SIZE_CROPPED,3])\n",
    "    distorted_image = tf.image.random_flip_left_right(distorted_image)\n",
    "    distorted_image = tf.image.random_brightness(distorted_image, max_delta=63)\n",
    "    distorted_image = tf.image.random_contrast(\n",
    "        distorted_image, lower=0.2, upper=1.8)\n",
    "    distorted_image = tf.image.per_image_standardization(distorted_image)\n",
    "    return distorted_image, label\n",
    "\n",
    "# parse testing data\n",
    "def map_fun_test(image,label):\n",
    "    image = tf.reshape(image,[IMAGE_DEPTH,IMAGE_HEIGHT,IMAGE_WIDTH])\n",
    "    image = tf.divide(tf.cast(tf.transpose(image,[1,2,0]),tf.float32),255.0)\n",
    "    label = tf.one_hot(label,10)\n",
    "    distorted_image = tf.image.resize_with_crop_or_pad(image, IMAGE_SIZE_CROPPED,IMAGE_SIZE_CROPPED)\n",
    "    distorted_image = tf.image.per_image_standardization(distorted_image)\n",
    "    return distorted_image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0343b85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = None\n",
    "Y_train = None\n",
    "X_test = None\n",
    "Y_test = None\n",
    "\n",
    "for filename in filenames_train:\n",
    "    images,labels = unpickle(filename)\n",
    "    X_train = images if X_train is None else np.concatenate((X_train,images))\n",
    "    Y_train = labels if Y_train is None else np.concatenate((Y_train,labels))\n",
    "\n",
    "for filename in filenames_test:\n",
    "    images,labels = unpickle(filename)\n",
    "    X_test = images if X_test is None else np.concatenate((X_test,images))\n",
    "    Y_test = labels if Y_test is None else np.concatenate((Y_test,labels))\n",
    "\n",
    "# Construct training Dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X_train,Y_train))\n",
    "dataset = dataset.map(map_fun)\n",
    "dataset = dataset.shuffle(10000)\n",
    "dataset = dataset.batch(64)\n",
    "\n",
    "# # Construct testing Dataset\n",
    "dataset_test = tf.data.Dataset.from_tensor_slices((X_test,Y_test))\n",
    "dataset_test = dataset_test.map(map_fun_test)\n",
    "dataset_test = dataset_test.batch(64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1554c92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cifar = models.Sequential()\n",
    "\n",
    "model_cifar.add(layers.Conv2D(64, (5, 5), padding='same', activation='relu', input_shape=(24, 24, 3)))\n",
    "model_cifar.add(layers.MaxPool2D(pool_size=3,strides=2,padding='same'))\n",
    "model_cifar.add(layers.BatchNormalization())\n",
    "                \n",
    "model_cifar.add(layers.Conv2D(64, (5, 5), padding='same', activation='relu'))\n",
    "model_cifar.add(layers.MaxPool2D(pool_size=3,strides=2,padding='same'))\n",
    "model_cifar.add(layers.BatchNormalization())\n",
    "                \n",
    "model_cifar.add(layers.Flatten())\n",
    "model_cifar.add(layers.Dense(384, activation='relu'))\n",
    "model_cifar.add(layers.Dropout(0.5))\n",
    "model_cifar.add(layers.Dense(192, activation='relu'))\n",
    "model_cifar.add(layers.Dropout(0.5))\n",
    "model_cifar.add(layers.Dense(10, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2618b742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x17e641d5130>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cifar.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_cifar.fit(dataset, epochs=200, validation_data=dataset_test, verbose=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d640118",
   "metadata": {},
   "source": [
    "We have done our training! Let's see whether our model is great or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "39e551dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 0.8522999882698059\n"
     ]
    }
   ],
   "source": [
    "_, test_acc = model_cifar.evaluate(dataset_test, verbose=0)\n",
    "print('test accuracy:',test_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8874318",
   "metadata": {},
   "source": [
    "## Using TFRecords\n",
    "To read data efficiently it can be helpful to serialize your data and store it in a set of files (100-200MB each) that can each be read linearly. This is especially true if the data is being streamed over a network. This can also be useful for caching any data-preprocessing.\n",
    "\n",
    "The TFRecord format is a simple format for storing a sequence of binary records.\n",
    "\n",
    "Protocol buffers are a cross-platform, cross-language library for efficient serialization of structured data.\n",
    "\n",
    "Protocol messages are defined by .proto files, these are often the easiest way to understand a message type.\n",
    "\n",
    "The tf.Example message (or protobuf) is a flexible message type that represents a {\"string\": value} mapping. It is designed for use with TensorFlow and is used throughout the higher-level APIs such as TFX.\n",
    "\n",
    "Here we will demonstrate how to create, parse, and use the tf.Example message, and then serialize, write, and read tf.Example messages to and from .tfrecord files.\n",
    "\n",
    "### tf.Example\n",
    "#### Data types for tf.Example:\n",
    "\n",
    "Fundamentally, a tf.Example is a {\"string\": tf.train.Feature} mapping.\n",
    "\n",
    "The tf.train.Feature message type can accept one of the following three types (See the .proto file for reference). Most other generic types can be coerced into one of these:\n",
    "\n",
    "1. tf.train.BytesList (the following types can be coerced)\n",
    "    * string\n",
    "    * byte\n",
    "2. tf.train.FloatList (the following types can be coerced)\n",
    "    * float (float32)\n",
    "    * double (float64)\n",
    "3. tf.train.Int64List (the following types can be coerced)\n",
    "    * bool\n",
    "    * enum\n",
    "    * int32\n",
    "    * uint32\n",
    "    * int64\n",
    "    * uint64\n",
    "    \n",
    "In order to convert a standard TensorFlow type to a tf.Example-compatible tf.train.Feature, you can use the shortcut functions below. Note that each function takes a scalar input value and returns a tf.train.Feature containing one of the three list types above:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cd65b961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following functions can be used to convert a value to a type compatible with tf.Example.\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    if isinstance(value, type(tf.constant(0))):\n",
    "        value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _float_feature(value):\n",
    "    \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ec0dd3",
   "metadata": {},
   "source": [
    "### Creating a tf.Example message\n",
    "Suppose you want to create a tf.Example message from existing data. In practice, the dataset may come from anywhere, but the procedure of creating the tf.Example message from a single observation will be the same:\n",
    "\n",
    "1. Within each observation, each value needs to be converted to a tf.train.Feature containing one of the 3 compatible types, using one of the functions above.\n",
    "\n",
    "2. You create a map (dictionary) from the feature name string to the encoded feature value produced in #1.\n",
    "\n",
    "3. The map produced in step 2 is converted to a Features message.\n",
    "\n",
    "In this notebook, you will create a dataset using NumPy.\n",
    "\n",
    "This dataset will have 4 features:\n",
    "\n",
    "* a boolean feature, False or True with equal probability\n",
    "* an integer feature uniformly randomly chosen from [0, 5]\n",
    "* a string feature generated from a string table by using the integer feature as an index\n",
    "* a float feature from a standard normal distribution\n",
    "Consider a sample consisting of 10,000 independently and identically distributed observations from each of the above distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0cfa456b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of samples in the dataset.\n",
    "n_samples_ = int(1e4)\n",
    "\n",
    "# Boolean feature, encoded as False or True.\n",
    "feature0 = np.random.choice([False, True], n_samples_)\n",
    "\n",
    "# Integer feature, random from 0 to 4.\n",
    "feature1 = np.random.randint(0, 5, n_samples_)\n",
    "\n",
    "# String feature\n",
    "strings = np.array([b'cat', b'dog', b'chicken', b'horse', b'goat'])\n",
    "feature2 = strings[feature1]\n",
    "\n",
    "# Float feature, from a standard normal distribution\n",
    "feature3 = np.random.randn(n_samples_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57425fbf",
   "metadata": {},
   "source": [
    "Each of these features can be coerced into a tf.Example-compatible type using one of **_bytes_feature, _float_feature, _int64_feature**. You can then create a tf.Example message from these encoded features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9734a35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_example(feature0, feature1, feature2, feature3):\n",
    "    \"\"\"\n",
    "    Creates a tf.Example message ready to be written to a file.\n",
    "    \"\"\"\n",
    "    # Create a dictionary mapping the feature name to the tf.Example-compatible data type.\n",
    "    feature = {\n",
    "        'feature0': _int64_feature(feature0),\n",
    "        'feature1': _int64_feature(feature1),\n",
    "        'feature2': _bytes_feature(feature2),\n",
    "        'feature3': _float_feature(feature3),\n",
    "    }\n",
    "\n",
    "    # Create a Features message using tf.train.Example.\n",
    "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    \n",
    "    return example_proto.SerializeToString()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a800a397",
   "metadata": {},
   "source": [
    "### TFRecord files using tf.data\n",
    "The tf.data module also provides tools for reading and writing data in TensorFlow.\n",
    "\n",
    "#### Writing a TFRecord file\n",
    "The easiest way to get the data into a dataset is to use the **from_tensor_slices** method.\n",
    "\n",
    "Applies to a tuple of arrays, it returns a dataset of tuples:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e678ba80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: ((), (), (), ()), types: (tf.bool, tf.int32, tf.string, tf.float64)>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_dataset = tf.data.Dataset.from_tensor_slices((feature0, feature1, feature2, feature3))\n",
    "features_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed88a8d3",
   "metadata": {},
   "source": [
    "Use the **tf.data.Dataset.map** method to apply a function to each element of a Dataset.\n",
    "\n",
    "The mapped function must operate in TensorFlow graph mode—it must operate on and return tf.Tensors. A non-tensor function, like serialize_example, can be wrapped with **tf.py_function** to make it compatible.\n",
    "\n",
    "Using tf.py_function requires to specify the shape and type information that is otherwise unavailable:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "71786cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_serialize_example(f0,f1,f2,f3):\n",
    "    tf_string = tf.py_function(\n",
    "        serialize_example,\n",
    "        (f0,f1,f2,f3),  # pass these args to the above function.\n",
    "        tf.string)      # the return type is `tf.string`.\n",
    "    return tf.reshape(tf_string, ()) # The result is a scalar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e53848e",
   "metadata": {},
   "source": [
    "Apply this function to each element in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bdfcc4a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: (), types: tf.string>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serialized_features_dataset = features_dataset.map(tf_serialize_example)\n",
    "serialized_features_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655f8d3d",
   "metadata": {},
   "source": [
    "And write them to a TFRecord file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8090af8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'test.tfrecord'\n",
    "writer = tf.data.experimental.TFRecordWriter(filename)\n",
    "writer.write(serialized_features_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37c3caf",
   "metadata": {},
   "source": [
    "#### Reading a TFRecord file\n",
    "You can also read the TFRecord file using the **tf.data.TFRecordDataset** class.\n",
    "\n",
    "More information on consuming TFRecord files using tf.data can be found [here](https://www.tensorflow.org/guide/data#consuming_tfrecord_data).\n",
    "\n",
    "Using TFRecordDataset can be useful for standardizing input data and optimizing performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "65af4a65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TFRecordDatasetV2 shapes: (), types: tf.string>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames = [filename]\n",
    "raw_dataset = tf.data.TFRecordDataset(filenames)\n",
    "raw_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f62cf4",
   "metadata": {},
   "source": [
    "At this point the dataset contains serialized **tf.train.Example** messages. When iterated over it returns these as scalar string tensors.\n",
    "\n",
    "Use the **.take** method to only show the first 3 records.\n",
    "\n",
    "* Note: iterating over a tf.data.Dataset only works with eager execution enabled.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "48d06d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Tensor: shape=(), dtype=string, numpy=b'\\nU\\n\\x11\\n\\x08feature1\\x12\\x05\\x1a\\x03\\n\\x01\\x02\\n\\x17\\n\\x08feature2\\x12\\x0b\\n\\t\\n\\x07chicken\\n\\x14\\n\\x08feature3\\x12\\x08\\x12\\x06\\n\\x04\\xec;p\\xbd\\n\\x11\\n\\x08feature0\\x12\\x05\\x1a\\x03\\n\\x01\\x01'>\n",
      "<tf.Tensor: shape=(), dtype=string, numpy=b'\\nU\\n\\x14\\n\\x08feature3\\x12\\x08\\x12\\x06\\n\\x04\\xd0.2\\xbf\\n\\x17\\n\\x08feature2\\x12\\x0b\\n\\t\\n\\x07chicken\\n\\x11\\n\\x08feature0\\x12\\x05\\x1a\\x03\\n\\x01\\x01\\n\\x11\\n\\x08feature1\\x12\\x05\\x1a\\x03\\n\\x01\\x02'>\n",
      "<tf.Tensor: shape=(), dtype=string, numpy=b'\\nQ\\n\\x14\\n\\x08feature3\\x12\\x08\\x12\\x06\\n\\x04\\xc0\\xaf\\x0c?\\n\\x11\\n\\x08feature0\\x12\\x05\\x1a\\x03\\n\\x01\\x00\\n\\x11\\n\\x08feature1\\x12\\x05\\x1a\\x03\\n\\x01\\x01\\n\\x13\\n\\x08feature2\\x12\\x07\\n\\x05\\n\\x03dog'>\n"
     ]
    }
   ],
   "source": [
    "for raw_record in raw_dataset.take(3):\n",
    "    print(repr(raw_record))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d819d80",
   "metadata": {},
   "source": [
    "These tensors can be parsed using the function below. Note that the **feature_description** is necessary here because datasets use graph-execution, and need this description to build their shape and type signature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a818c7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a description of the features.\n",
    "feature_description = {\n",
    "    'feature0': tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
    "    'feature1': tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
    "    'feature2': tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
    "    'feature3': tf.io.FixedLenFeature([], tf.float32, default_value=0.0),\n",
    "}\n",
    "\n",
    "def _parse_function(example_proto):\n",
    "  # Parse the input `tf.Example` proto using the dictionary above.\n",
    "  return tf.io.parse_single_example(example_proto, feature_description)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c66120",
   "metadata": {},
   "source": [
    "Alternatively, use **tf.parse example** to parse the whole batch at once. Apply this function to each item in the dataset using the tf.data.Dataset.map method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ced059fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: {feature0: (), feature1: (), feature2: (), feature3: ()}, types: {feature0: tf.int64, feature1: tf.int64, feature2: tf.string, feature3: tf.float32}>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_dataset = raw_dataset.map(_parse_function)\n",
    "parsed_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0273cd",
   "metadata": {},
   "source": [
    "Use eager execution to display the observations in the dataset. There are 10,000 observations in this dataset, but you will only display the first 3. The data is displayed as a dictionary of features. Each item is a **tf.Tensor**, and the **numpy** element of this tensor displays the value of the feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c947f7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'feature0': <tf.Tensor: shape=(), dtype=int64, numpy=1>, 'feature1': <tf.Tensor: shape=(), dtype=int64, numpy=2>, 'feature2': <tf.Tensor: shape=(), dtype=string, numpy=b'chicken'>, 'feature3': <tf.Tensor: shape=(), dtype=float32, numpy=-0.058650896>}\n",
      "{'feature0': <tf.Tensor: shape=(), dtype=int64, numpy=1>, 'feature1': <tf.Tensor: shape=(), dtype=int64, numpy=2>, 'feature2': <tf.Tensor: shape=(), dtype=string, numpy=b'chicken'>, 'feature3': <tf.Tensor: shape=(), dtype=float32, numpy=-0.6960268>}\n",
      "{'feature0': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'feature1': <tf.Tensor: shape=(), dtype=int64, numpy=1>, 'feature2': <tf.Tensor: shape=(), dtype=string, numpy=b'dog'>, 'feature3': <tf.Tensor: shape=(), dtype=float32, numpy=0.54955673>}\n"
     ]
    }
   ],
   "source": [
    "for parsed_record in parsed_dataset.take(3):\n",
    "    print(repr(parsed_record))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b388057c",
   "metadata": {},
   "source": [
    "Here, the tf.parse_example function unpacks the tf.Example fields into standard tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a76976",
   "metadata": {},
   "source": [
    "### TFRecord files in Python\n",
    "The **tf.io** module also contains pure-Python functions for reading and writing TFRecord files.\n",
    "\n",
    "### Writing a TFRecord file\n",
    "Next, write the 10,000 observations to the file **test.tfrecord**. Each observation is converted to a **tf.Example** message, then written to file. You can then verify that the file **test.tfrecord** has been created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0aaac07b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ginag\\AppData\\Local\\Temp/ipykernel_6480/2117107812.py:15: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n"
     ]
    }
   ],
   "source": [
    "# Write the `tf.Example` observations to the file.\n",
    "with tf.io.TFRecordWriter(filename) as writer:\n",
    "    for i in range(n_samples_):\n",
    "        example = serialize_example(feature0[i], feature1[i], feature2[i], feature3[i])\n",
    "        writer.write(example)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855408a1",
   "metadata": {},
   "source": [
    "### Reading a TFRecord file\n",
    "These serialized tensors can be easily parsed using tf.train.Example.ParseFromString:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c38dae93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TFRecordDatasetV2 shapes: (), types: tf.string>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames = [filename]\n",
    "raw_dataset = tf.data.TFRecordDataset(filenames)\n",
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "23407c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features {\n",
      "  feature {\n",
      "    key: \"feature0\"\n",
      "    value {\n",
      "      int64_list {\n",
      "        value: 1\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"feature1\"\n",
      "    value {\n",
      "      int64_list {\n",
      "        value: 2\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"feature2\"\n",
      "    value {\n",
      "      bytes_list {\n",
      "        value: \"chicken\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"feature3\"\n",
      "    value {\n",
      "      float_list {\n",
      "        value: -0.058650895953178406\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for raw_record in raw_dataset.take(1):\n",
    "    example = tf.train.Example()\n",
    "    example.ParseFromString(raw_record.numpy())\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d84ea77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-gpu",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
